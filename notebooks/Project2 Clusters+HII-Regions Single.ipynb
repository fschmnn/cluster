{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Single <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues. This notebook is for use with a single galaxy at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pnlf.packages import *\n",
    "\n",
    "from pnlf.constants import tab10, single_column, two_column\n",
    "from pnlf.plot import quick_plot, add_scale\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,datefmt='%H:%M:%S',level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "basedir = Path('..')  # where we save stuff (and )\n",
    "data_ext = Path('a:') # raw data\n",
    "\n",
    "# we use the sample table for basic galaxy properties\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('Name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "the galaxies listed in `hst_sample` have a cluster catalogue. The galaxies listed in `muse_sample` have astrosat observations to measure the FUV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_sample      = set(['NGC0628', 'NGC1433', 'NGC1566', 'NGC3351', 'NGC3627', 'NGC4535'])\n",
    "astrosat_sample = set([x.stem.split('_')[0] for x in (data_ext/'Astrosat').iterdir() if x.is_file() and x.suffix=='.fits'])\n",
    "muse_sample     = set(sample_table['Name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample\n",
    "\n",
    "name = 'NGC1566'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE (DAP + nebulae catalogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import filter_table\n",
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "# DAP linemaps (Halpha and OIII)\n",
    "filename = data_ext / 'MUSE_DR2' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                    meta=hdul['OIII5006_FLUX'].header,\n",
    "                    wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "path = data_ext / 'MUSE_DR2' / 'filterImages' \n",
    "sdss_g, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_g_WCS_Pall_mad.fits',header=True)\n",
    "sdss_r, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_r_WCS_Pall_mad.fits',header=True)\n",
    "sdss_i, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_i_WCS_Pall_mad.fits',header=True)\n",
    "    \n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "filename = basedir / 'data' / 'interim' / f'Nebulae_Catalogue_with_FUV_DR2.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg)\n",
    "nebulae.rename_columns(['cen_x','cen_y'],['x','y'])\n",
    "nebulae.add_index('region_ID')\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "    nebulae[SII>0]['[SIII]/[SII]'] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    \n",
    "HII_regions = filter_table(nebulae,gal_name=name,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "nebulae = filter_table(nebulae,gal_name=name)\n",
    "\n",
    "filename = data_ext / 'MUSE_DR2' / 'Nebulae catalogue' /'spatial_masks'/f'{name}_HIIreg_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data-1,mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "# WFI image (larger FOV)\n",
    "filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    WFI = NDData(data=hdul[0].data,\n",
    "                 meta=hdul[0].header,\n",
    "                 wcs=WCS(hdul[0].header))\n",
    "    \n",
    "# most of the time we do not need the datacubes\n",
    "if False:\n",
    "    #from spectral_cube import SpectralCube\n",
    "    filename = Path('g:') /'Archive'/'MUSE'/'DR2'/'datacubes'/f'{name}_DATACUBE_FINAL_WCS_Pall_mad.fits'\n",
    "    with fits.open(filename , memmap=True, mode='denywrite') as hdul:\n",
    "        data_cube   = hdul[1].data\n",
    "        cube_header = hdul[1].header   \n",
    "    \n",
    "print(f'{name}: {len(HII_regions)} HII-regions in final catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HST\n",
    "\n",
    "**white light + filter images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitelight image (we set 0s to nan)\n",
    "with fits.open(data_ext / 'HST' / 'white_light' / f'{name.lower()}_white_24rgb.fits') as hdul:\n",
    "    hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "    \n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_exp_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_err_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275.uncertainty = StdDevUncertainty(hdul[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cluster and associations catalogues**\n",
    "\n",
    "the following 7 galaxies will be available (end of february)\n",
    "\n",
    "NGC 628, NGC 1365, NGC 1433, NGC 1566, NGC 3351, NGC 3627, NGC 4535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select resolution for association catlaogue\n",
    "res = 32\n",
    "\n",
    "# cluster catalogues\n",
    "filename = data_ext / 'HST' / 'cluster catalogue' / f'{name}_phangshst_base_catalog.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    clusters = Table(hdul[1].data)\n",
    "clusters['SkyCoord'] = SkyCoord(clusters['PHANGS_RA']*u.degree,clusters['PHANGS_DEC']*u.degree)\n",
    "\n",
    "# remove LEGUS columns\n",
    "clusters = clusters[[x for x in clusters.columns if 'LEGUS' not in x]]\n",
    "# remove the PHANGS label from the column names\n",
    "clusters.rename_columns([x for x in clusters.columns],[x.replace('PHANGS_','') for x in clusters.columns])\n",
    "# we rename the columns such that the clusters and associations are identical\n",
    "clusters.rename_columns(['ID_CLUSTERS_V0_9','AGE_MINCHISQ','MASS_MINCHISQ','EBV_MINCHISQ',\n",
    "                         'AGE_MINCHISQ_ERR','MASS_MINCHISQ_ERR','EBV_MINCHISQ_ERR'],\n",
    "                        ['cluster_ID','age','mass','EBV','age_err','mass_err','EBV_err'])\n",
    "# only CLASS 1,2 and 3 are classified as clusters\n",
    "clusters = filter_table(clusters,CLUSTER_CLASS=[1,2,3])\n",
    "print(f'{name}: {len(clusters)} clusters in final catalogue')    \n",
    "\n",
    "# association catalogue\n",
    "filename = data_ext / 'HST' / 'associations' / name / 'associations' / f'{name.lower()}_phangshst_associations_nuv_ws{res}pc_v0_9.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    associations = Table(hdul[1].data)\n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)\n",
    "# we rename the columns such that the clusters and associations are identical\n",
    "associations.rename_columns(['reg_id','reg_ra','reg_dec','reg_x','reg_y',\n",
    "                             'reg_dolflux_Age_MinChiSq','reg_dolflux_Mass_MinChiSq','reg_dolflux_Ebv_MinChiSq',\n",
    "                             'reg_dolflux_Age_MinChiSq_err','reg_dolflux_Mass_MinChiSq_err','reg_dolflux_Ebv_MinChiSq_err'],\n",
    "                            ['cluster_ID','RA','DEC','X','Y','age','mass','EBV','age_err','mass_err','EBV_err'])\n",
    "\n",
    "# associations mask\n",
    "filename = data_ext / 'HST' / 'associations' / name / 'associations' / f'{name.lower()}_phangshst_associations_nuv_ws{res}pc_idmask_v0_9.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    data = hdul[0].data.astype(float)\n",
    "    data[data==0] = np.nan\n",
    "    associations_mask = NDData(data,\n",
    "                               mask=data==0,\n",
    "                               meta=hdul[0].header,\n",
    "                               wcs=WCS(hdul[0].header))\n",
    "print(f'{name}: {len(associations)} associations in catalogue')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Astrosat\n",
    "\n",
    "https://uvit.iiap.res.in/Instrument/Filters\n",
    "\n",
    "the resolution is 0.4\" per pixel. With a PSF resolution of 1.8\" this leads to fwhm ~ 4.5 px. This corresponds to a std = 1.91 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {name}')\n",
    "    \n",
    "with fits.open(astro_file) as hdul:\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalent Width\n",
    "\n",
    "the first step is to extract the spectra of each HII-region.\n",
    "\n",
    "Are the spectra continuum subtracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'{name}_VorSpectra.fits') as hdul:\n",
    "    spectra = Table(hdul[1].data)\n",
    "    spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    \n",
    "spectra['region_ID'] = np.arange(len(spectra))\n",
    "spectra.add_index('region_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = 67 * u.km / u.s / u.Mpc\n",
    "z = (H0*Distance(distmod=p['(m-M)'])/c.c).decompose()\n",
    "lam_HA0 = 6562.8*u.Angstrom\n",
    "lam_HA = (1+z)*lam_HA0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import models, fitting\n",
    "\n",
    "def fit_emission_line(spectral_axis,flux,lam0,plot=False):\n",
    "    \n",
    "    region = (spectral_axis>lam0-10*u.Angstrom) & (spectral_axis<lam0+15*u.Angstrom) \n",
    "    amplitude_guess = np.max(flux[region].value)\n",
    "    \n",
    "    # create a model with a gaussian + a constant \n",
    "    model = models.Gaussian1D(amplitude_guess, lam0.value, 1) + models.Polynomial1D(degree=0)\n",
    "    model.amplitude_0.min = 0\n",
    "    model.mean_0.bounds = (lam0.value-3,lam0.value+3)\n",
    "    fitter = fitting.LevMarLSQFitter()\n",
    "    fit = fitter(model, spectral_axis[region].value, flux[region].value)\n",
    "    \n",
    "    integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "    continuum = fit.c0_1 * u.erg/u.s/u.cm**2/u.Angstrom\n",
    "    eq_width = integrated_flux/continuum\n",
    "    \n",
    "    if plot:\n",
    "        f, ax = plt.subplots()  \n",
    "        ax.scatter(spectral_axis.value, flux.value,color='black',s=2) \n",
    "        ax.plot(spectral_axis[region].value,fit(spectral_axis[region].value),color='tab:red')\n",
    "        ax.plot(spectral_axis.value,0*spectral_axis.value+fit.c0_1,ls='--',color='tab:red')\n",
    "        ax.axvline(lam0,color='black',ls='--')\n",
    "        ax.fill_between(spectral_axis,fit(spectral_axis.value),0*spectral_axis.value+fit.c0_1,alpha=0.5)\n",
    "        \n",
    "        eq_region = (spectral_axis>lam0+7*u.Angstrom) & (spectral_axis<lam0+7*u.Angstrom+eq_width) \n",
    "        ax.fill_between(spectral_axis[eq_region],0*spectral_axis[eq_region].value+fit.c0_1,0*spectral_axis[eq_region].value,alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim(lam0-30*u.Angstrom, lam0+30*u.Angstrom) \n",
    "        plt.show()\n",
    "        \n",
    "    return fit\n",
    "\n",
    "region_ID = 12\n",
    "lam_HA = 6595*u.Angstrom\n",
    "flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "fit = fit_emission_line(spectral_axis,flux,lam_HA,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_HA = 6595*u.Angstrom\n",
    "\n",
    "HA = []\n",
    "HII_regions['eq_width'] = np.nan\n",
    "for region_ID in HII_regions['region_ID']:\n",
    "    \n",
    "    flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "    fit = fit_emission_line(spectral_axis,flux,lam_HA,plot=False)\n",
    "    integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "    continuum = fit.c0_1 * u.erg/u.s/u.cm**2/u.Angstrom\n",
    "    eq_width = integrated_flux/continuum\n",
    "    #if eq_width.value<0: eq_width = np.nan\n",
    "    HII_regions.loc[region_ID]['eq_width'] = eq_width.value\n",
    "    \n",
    "    HA.append(integrated_flux)\n",
    "    #HA_cat = nebulae.loc[region_ID]['HA6562_FLUX']\n",
    "    #print(f'{integrated_flux/HA_cat:.2f}')\n",
    "    #print(f'HA = {fit.mean_0.value:.2f}')\n",
    "HA = np.array([x.value for x in HA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(HA,HII_regions['HA6562_FLUX'])\n",
    "x = np.linspace(0,1.2e7)\n",
    "plt.plot(x,1/0.4*x,color='black')\n",
    "plt.ylim([0,3.2e7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HST and MUSE\n",
    "\n",
    "### compare footprints of different observations\n",
    "\n",
    "here we compare the footprints of the observations and check which objects overlap. The FOV of astrosat is a circle with a diameter of 28'. This is much larger than HST and MUSE and both will always be covered by the astrosat observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "\n",
    "# check which nebulae/clusters are within the HST/MUSE FOV\n",
    "associations['in_frame'] = reg_muse_sky.contains(associations['SkyCoord'],nebulae_mask.wcs)\n",
    "clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "nebulae['in_frame']  = reg_hst_sky.contains(nebulae['SkyCoord'],nebulae_mask.wcs)\n",
    "\n",
    "print(f'{np.sum(associations[\"in_frame\"])} (of {len(associations)}) associations in MUSE FOV')\n",
    "print(f'{np.sum(clusters[\"in_frame\"])} (of {len(clusters)}) clusters in MUSE FOV')\n",
    "print(f'{np.sum(nebulae[\"in_frame\"])} (of {len(nebulae)}) nebulae in HST FOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=6*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "ax = quick_plot(WFI_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/'footpring.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association and nebulae\n",
    "\n",
    "the association catalogue differs from the clusters in that its entries are extended. Because we match two catalogues with extended objects, we must proceed differently.\n",
    "\n",
    "In a first step we take a look at a single association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "cluster_ID = 42\n",
    "pos = associations['SkyCoord'][associations['cluster_ID']==cluster_ID]\n",
    "\n",
    "contours = find_contours(associations_mask.data==cluster_ID,0.5,)\n",
    "coords = max(contours,key=len)\n",
    "\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_pix  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_sky  = reg_pix.to_sky(associations_mask.wcs)\n",
    "\n",
    "mask_cutout = Cutout2D(associations_mask.data,pos,size=1*u.arcsecond,wcs=associations_mask.wcs)\n",
    "F275_cutout = Cutout2D(F275.data,pos,size=2*u.arcsecond,wcs=F275.wcs)\n",
    "\n",
    "reg_pix_cut  = reg_sky.to_pixel(mask_cutout.wcs)\n",
    "\n",
    "ax = quick_plot(F275_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "ax.imshow(mask_cutout.data,alpha=0.5)\n",
    "\n",
    "reg_pix_cut.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to match the nebulae to the associations we first reproject the mask of the nebulae to the HST image. We then scale the association mask by the number of associations (assume we have 1432 objects, then 615 becomes 0.0615). This way we can add the two masks together and infer from the resulting unique values which clusters overlap with which associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "# reproject nebulae mask to hst \n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=associations_mask.wcs,\n",
    "                                   shape_out=associations_mask.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "\n",
    "scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "s_arr = associations_mask.data/scale+nebulae_hst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids of associations, nebulae and combination (sum) of both\n",
    "a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "n_id = np.unique(nebulae_hst[~np.isnan(nebulae_hst)]).astype(int)\n",
    "s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "# this splits the sum into two parts (nebulae and associations)\n",
    "a_modf,n_modf = np.modf(s_id)\n",
    "n_modf = n_modf.astype(int)\n",
    "a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "nebulae_dict = {n : list(a_modf[n_modf==n]) for n in n_id}     \n",
    "associations_dict = {a : list(n_modf[a_modf==a]) for a in a_id}     \n",
    "\n",
    "# so far we ensured that the nebulae in unique_n have only one association,\n",
    "# but it is possible that this association goes beyond the nebulae and into\n",
    "# a second nebulae. Those objects are excluded here\n",
    "isolated_nebulae = []\n",
    "isolated_assoc   = []\n",
    "for n,v in nebulae_dict.items():\n",
    "    if len(v)==1:\n",
    "        if len(associations_dict[v[0]])==1:\n",
    "            isolated_nebulae.append(n)\n",
    "            isolated_assoc.append(v[0])\n",
    "            \n",
    "print(f'n_associations = {len(a_id)}')\n",
    "print(f'n_nebulae      = {len(n_id)}')\n",
    "print(f'1to1 match     = {len(isolated_nebulae)}')\n",
    "\n",
    "hist,bins,_=plt.hist(count_n,bins=np.arange(0.5,10.5,1),width=0.8)\n",
    "plt.title('all objects')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import single_cutout\n",
    "\n",
    "region_ID = 7\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "single_cutout(ax=ax,position = position,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             #label= f'{region_ID}/{nebulae_dict[region_ID][0]}',\n",
    "             size = 4*u.arcsecond)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'single_region.png',dpi=315)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot a cutout and see if the match is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join catalogues\n",
    "\n",
    "and make some plots that showcase the position/overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside = []\n",
    "for a in isolated_assoc:\n",
    "    if np.any(np.isnan(nebulae_hst[associations_mask.data==a])):\n",
    "        outside.append(True)\n",
    "    else:\n",
    "        outside.append(False)\n",
    "outside=np.array(outside)\n",
    "print(f'{np.sum(~outside)} associations fall completly insde one HII-region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "# assign to each association the ID of the nebulae \n",
    "assoc_tmp = associations[np.searchsorted(associations['cluster_ID'], isolated_assoc)].copy()\n",
    "assoc_tmp['region_ID'] = isolated_nebulae\n",
    "assoc_tmp['contained'] = ~outside\n",
    "nebulae_tmp = HII_regions[np.isin(HII_regions['region_ID'],assoc_tmp['region_ID'])]\n",
    "if 'eq_width' not in nebulae_tmp.columns:\n",
    "    nebulae_tmp['eq_width'] = np.nan\n",
    "\n",
    "catalogue = join(assoc_tmp,nebulae_tmp,keys='region_ID')\n",
    "catalogue.rename_columns(['X','Y','x','y','RA','DEC','cen_ra','cen_dec','reg_area','region_area',\n",
    "                          'EBV_1','EBV_2','EBV_err','EBV_ERR','SkyCoord_1','SkyCoord_2'],\n",
    "                         ['x_asc','y_asc','x_neb','y_neb','ra_asc','dec_asc','ra_neb','dec_neb',\n",
    "                          'area_asc','area_neb','EBV_stars','EBV_balmer','EBV_stars_err','EBV_balmer_err',\n",
    "                          'SkyCoord_asc','SkyCoord_neb'])\n",
    "\n",
    "# select the columns of the joined catalogue\n",
    "columns = ['cluster_ID','region_ID','x_asc','y_asc','x_neb','y_neb',\n",
    "           'ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_asc','SkyCoord_neb','area_asc','area_neb',\n",
    "           'age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','EBV_balmer','EBV_balmer_err',\n",
    "           'met_scal','met_scal_err','logq_D91','logq_D91_err',] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR')] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR_ERR')] + \\\n",
    "            ['HA/FUV','contained','eq_width']\n",
    "catalogue = catalogue[columns]\n",
    "        \n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR')],\n",
    "                      [col.replace('FLUX_CORR','flux') for col in catalogue.columns if col.endswith('FLUX_CORR')])\n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')],\n",
    "                      [col.replace('FLUX_CORR_ERR','flux_err') for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')])\n",
    "catalogue['cluster_ID'] = catalogue['cluster_ID'].astype('int')\n",
    "catalogue['region_ID'] = catalogue['region_ID'].astype('int')\n",
    "\n",
    "catalogue.info.description = 'Joined catalogue between associations and nebulae'\n",
    "mean_sep = np.mean(catalogue['SkyCoord_asc'].separation(catalogue['SkyCoord_neb']))\n",
    "print(f'{len(catalogue)} objects in catalogue')\n",
    "print(f'the mean separation between cluster and association center is {mean_sep.to(u.arcsecond):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the joined catalogue (containing only nebulae and clusters with a 1 to 1 relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_cutout\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = catalogue['SkyCoord_neb'][catalogue['contained']]\n",
    "labels = [f'{ri}/{ci}' for ri, ci in catalogue[['region_ID','cluster_ID']][catalogue['contained']]]\n",
    "\n",
    "multi_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a multi page pdf with all isolated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = catalogue['SkyCoord_neb'][catalogue['contained']]\n",
    "labels = [f'{ri}/{ci}' for ri, ci in catalogue[['region_ID','cluster_ID']][catalogue['contained']]]\n",
    "\n",
    "multi_page_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=4*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column),subplot_kw={'projection': WFI_cutout.wcs})\n",
    "norm = simple_norm(WFI_cutout.data,clip=False,percent=96)\n",
    "ax.imshow(WFI_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "x,y = catalogue['SkyCoord_neb'][catalogue['contained']].to_pixel(WFI_cutout.wcs)\n",
    "ax.scatter(x,y,marker='s',facecolors='none',s=30,lw=1,color='tab:red')\n",
    "\n",
    "ax.coords[0].set_ticks_visible(False)\n",
    "ax.coords[1].set_ticks_visible(False)\n",
    "ax.coords[0].set_ticklabel_visible(False)\n",
    "ax.coords[1].set_ticklabel_visible(False)\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export parts of the joined catalogue (right now only the fully contained objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = catalogue[catalogue['contained']]\n",
    "export.add_column(export['SkyCoord_asc'].to_string(style='hmsdms',precision=2),index=6,name='RaDec_asc')\n",
    "export.add_column(export['SkyCoord_neb'].to_string(style='hmsdms',precision=2),index=8,name='RaDec_neb')\n",
    "\n",
    "for col in export.columns:\n",
    "    if col not in ['RaDec_asc','RaDec_neb','region_ID','cluster_ID']:\n",
    "        export[col].info.format = '%.2f'\n",
    "\n",
    "del export[['ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_neb','SkyCoord_asc','contained','HA/FUV']]\n",
    "\n",
    "filename = basedir/'data'/'interim'/f'{name}_associations_and_nebulae_joined.txt'\n",
    "with open(filename,'w',newline='\\n') as f:\n",
    "    ascii.write(export,f,format='fixed_width_two_line',overwrite=True,delimiter_pad=' ',position_char='=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in joined catalogue\n",
    "\n",
    "first we create a mask to select a subset of objects (e.g. based on mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "# separation to other associations\n",
    "idx,sep_others,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_int,_= match_coordinates_sky(catalogue['SkyCoord_asc'],catalogue['SkyCoord_neb'])\n",
    "\n",
    "# size of the association compared to the HII-region\n",
    "small_HII = (catalogue['area_neb']/0.039) / (catalogue['area_asc']/11.95) > 2\n",
    "\n",
    "# distance to centre of galaxy\n",
    "galactic_center = SkyCoord(ra=p['R.A.'],dec=p['Dec.'])\n",
    "catalogue['galactic_radius'] = catalogue['SkyCoord_neb'].separation(galactic_center).to(u.arcmin)\n",
    "\n",
    "# define the criteria which objects we use in the plot\n",
    "criteria = (catalogue['mass']>1e4)  #& (catalogue['age']>catalogue['age_err'])\n",
    "\n",
    "#& catalogue['contained'] #& (catalogue['galactic_radius']>1*u.arcmin)\n",
    "\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HA/FUV vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(tmp['age'],tmp['HA/FUV']) #,c=tmp['mass'],vmin=1e5,vmax=3e5)\n",
    "#fig.colorbar(sc,label='mass / Msun')\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equivalent width vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(tmp['age'],tmp['eq_width']) #,c=tmp['mass'],vmin=1e5,vmax=3e5)\n",
    "#fig.colorbar(sc,label='mass / Msun')\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['eq_width'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='equivalent width / Angstrom',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_eq_width_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,100]\n",
    "criteria = HII_regions['HA/FUV']<150\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria])\n",
    "\n",
    "x,mean,std = bin_stat(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='equivalent width / Angstrom',ylabel='Halpha / FUV',xlim=xlim,ylim=(0,70))\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_eq_width.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extinction from stars and from nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# we expect EBV_balmer = 2 EBV_stars\n",
    "\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=plt.cm.viridis_r)\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_EBV_Balmer_vs_Stars.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_corner'\n",
    "columns  = ['age','eq_width','HA/FUV','met_scal','logq_D91']\n",
    "limits   = {'age':(0,10),'eq_width':(0,100),'HA/FUV':(0,50),'met_scal':(8.4,8.7),'logq_D91':(6,8)}\n",
    "\n",
    "corner(catalogue,columns,limits,filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "lines = [col for col in catalogue.columns if col.endswith('_flux')]\n",
    "print(f'{len(lines)} different lines with {comb(len(lines),2)} possible combinations')\n",
    "\n",
    "correlation = []\n",
    "for pair in itertools.combinations(lines,2):\n",
    "    not_nan = ~np.isnan(catalogue[pair[0]]) & ~np.isnan(catalogue[pair[1]])\n",
    "    r,p = spearmanr(catalogue['age'],catalogue[pair[0]][not_nan]/catalogue[pair[1]][not_nan])\n",
    "    correlation.append((r,pair))\n",
    "a = [x for x in correlation if np.abs(x[0])>0.15]\n",
    "a.sort(key=lambda x: np.abs(x[0]),reverse=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['HA/SII'] = catalogue['HA6562_flux'] / catalogue['SII6716_flux']\n",
    "catalogue['HA/OI'] = catalogue['HA6562_flux'] / catalogue['OI6300_flux']\n",
    "\n",
    "catalogue['HA/SII'][~np.isfinite(catalogue['HA/SII'])] = np.nan\n",
    "catalogue['HA/OI'][~np.isfinite(catalogue['HA/OI'])] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halpha luminosity vs mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1,5e5]\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] \n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['mass'],tmp['HA6562_FLUX'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['HA6562_FLUX'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='Halpha',xlim=xlim,ylim=[0,500000])\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1,2e5]\n",
    "criteria = (catalogue['age']<20) #& (sep>Angle('3\"'))\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(tmp['mass'],tmp['region_area'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['region_area'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='HII-region area',xlim=xlim)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters and Nebulae\n",
    "\n",
    "we match the position of the cluster catalogue to the nebulae catalogue\n",
    "\n",
    "we look up the value (=region_ID) in the spatial nebulae mask for each position in the cluster catalogue and asign this value to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data,x,y):\n",
    "    shape = data.shape\n",
    "    \n",
    "    out = []\n",
    "    for j,i in zip(x,y):\n",
    "        if 0<i<shape[0] and 0<j<shape[1]:\n",
    "            out.append(data[int(i),int(j)])\n",
    "        else:\n",
    "            out.append(np.nan)\n",
    "    return out\n",
    "\n",
    "clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "clusters['MUSE_X'],clusters['MUSE_Y'] = clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)\n",
    "\n",
    "clusters['region_ID'] = np.nan\n",
    "clusters['region_ID'][clusters['in_frame']] = get_value(nebulae_mask.data,x=clusters['MUSE_X'][clusters['in_frame']],y=clusters['MUSE_Y'][clusters['in_frame']])\n",
    "# we exclude clusters in PNe and SNRs \n",
    "clusters['region_ID'][~np.isin(clusters['region_ID'],HII_regions['region_ID'])] = np.nan\n",
    "\n",
    "region_ID = clusters['region_ID']\n",
    "print(f'{np.sum(~np.isnan(region_ID))} inside and {np.sum(np.isnan(region_ID))} outside of nebulae')\n",
    "unique, counts = np.unique(region_ID[~np.isnan(region_ID)],return_counts=True)\n",
    "isolated_clusters = clusters[np.isin(clusters['region_ID'],unique[counts==1])]\n",
    "print(f'{len(isolated_clusters)} isolated clusters (no other objects in nebulae)')\n",
    "\n",
    "hist,bins,_=plt.hist(counts,bins=np.arange(0.5,10.5,1),width=0.8)\n",
    "plt.title('all objects')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cutout of single region\n",
    "\n",
    "this function plots a cutout for one region in different filters. Overplotted are the detected clusters and nebulae (green circles are unclassified clusters and blue circles are classified clusters. Solid regions are HII-regions and dashed regions are other emission line nebulae like PNe or SNRs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37,77\n",
    "position = HII_regions['SkyCoord'][HII_regions['region_ID']==183][0]\n",
    "#position = isolated_clusters['SkyCoord'][0]\n",
    "#position = HII_regions['SkyCoord'][np.isin(HII_regions['region_ID'],hst_all_objects['region_ID'])][39]\n",
    "filename = basedir/'reports'/name/f'{name}_region'\n",
    "\n",
    "plot_cluster_nebulae(name,position,8*u.arcsec,\n",
    "                     F275,Halpha,astrosat,\n",
    "                     sdss_g,sdss_r,sdss_i,\n",
    "                     reg_hst_sky,nebulae_mask,\n",
    "                     associations_mask,\n",
    "                     HII_regions,\n",
    "                     filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis we require nebulae that are clearly associated with one cluster. Here we plot a cutout for each of those clusters with the detected nebulae and clusters overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import plot_cutouts\n",
    "\n",
    "#nebulae.sort(['HA6562_FLUX'])\n",
    "\n",
    "sample = isolated_clusters[:20]\n",
    "sample.sort('region_ID')\n",
    "filename = basedir/'reports'/name/f'isolated_clusters_F275_{name}'\n",
    "positions = sample['SkyCoord']\n",
    "labels = sample['region_ID']\n",
    "\n",
    "plot_cutouts(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             points= clusters,\n",
    "             labels= labels,\n",
    "             filename=filename,\n",
    "             ncols=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we plot the position of those \"isolated clusters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot import create_RGB\n",
    "\n",
    "fig = plt.figure(figsize=(two_column,two_column))\n",
    "ax1  = fig.add_subplot(111,projection=Halpha.wcs)\n",
    "\n",
    "gri = create_RGB(sdss_i,sdss_r,sdss_g,percentile=[98,98,98])\n",
    "\n",
    "# show image of the entire galaxy\n",
    "ax1.imshow(gri)\n",
    "reg_hst_muse  = reg_hst_sky.to_pixel(Halpha.wcs)\n",
    "reg_hst_muse.plot(ax=ax1,ec='tab:red',label='HST',lw=0.5)\n",
    "x,y = isolated_clusters['SkyCoord'].to_pixel(Halpha.wcs)\n",
    "ax1.scatter(x,y,marker='s',facecolors='none',s=50,lw=1,color='tab:red')\n",
    "#plt.savefig(basedir/'reports'/name/f'{name}_location_isolated_clusters.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the two catalgoues\n",
    "\n",
    "run only when something has changed and you want to save a new catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "    \n",
    "clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "clusters['MUSE_X'],clusters['MUSE_Y'] = clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)\n",
    "\n",
    "clusters['region_ID'] = np.nan\n",
    "clusters['region_ID'][clusters['in_frame']] = get_value(nebulae_mask.data,x=clusters['MUSE_X'][clusters['in_frame']],y=clusters['MUSE_Y'][clusters['in_frame']])\n",
    "# we exclude clusters in PNe and SNRs \n",
    "clusters['region_ID'][~np.isin(clusters['region_ID'],HII_regions['region_ID'])] = np.nan\n",
    "\n",
    "region_ID = clusters['region_ID']\n",
    "print(f'{np.sum(~np.isnan(region_ID))} inside and {np.sum(np.isnan(region_ID))} outside of nebulae')\n",
    "unique, counts = np.unique(region_ID[~np.isnan(region_ID)],return_counts=True)\n",
    "isolated_clusters = clusters[np.isin(clusters['region_ID'],unique[counts==1])]\n",
    "print(f'{len(isolated_clusters)} isolated clusters (no other objects in nebulae)')\n",
    "\n",
    "isolated_nebulae = nebulae[np.isin(nebulae['region_ID'],isolated_clusters['region_ID'])]\n",
    "\n",
    "catalogue = join(isolated_clusters,isolated_nebulae,keys='region_ID')\n",
    "#catalogue['met_scal'][0]\n",
    "\n",
    "del catalogue[['SkyCoord_1','SkyCoord_2']]\n",
    "\n",
    "# write to file\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(catalogue)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "#hdul.writeto(basedir/'data'/'interim'/f'{name}_Clusters+Nebulae.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in combined catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [0,20]\n",
    "tmp = catalogue[catalogue['mass']>5e3]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(tmp['age'],tmp['HA/FUV'])\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=[0,20])\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = basedir/'reports'/f'{name}_age_over_SII.pdf'\n",
    "fig, ax = plt.subplots(figsize=(single_column,single_column))\n",
    "ax.scatter(catalogue['age'],catalogue['[%%SVG]'])\n",
    "ax.set(xlim=(0,12),xlabel='age/Myr',ylabel='Ha/FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure FUV and Halpha at association position\n",
    "\n",
    "### From point\n",
    "\n",
    "If I select an aperture smaller than a pixel, the measured flux is directly proportional to the apertuer size. Therefore it doesn't matter that the astrosat resolution is much worse than HST or MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = CCM89(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyCircularAperture,SkyCircularAnnulus,aperture_photometry\n",
    "\n",
    "criteria = np.isin(associations['cluster_ID'],isolated_assoc)\n",
    "\n",
    "aperture_size = 1*u.arcsecond\n",
    "positions = associations['SkyCoord'][criteria]\n",
    "\n",
    "aperture = SkyCircularAperture(positions,aperture_size)\n",
    "\n",
    "fluxes = associations[['cluster_ID','SkyCoord','age','age_err','mass','mass_err','EBV','EBV_err']][criteria]\n",
    "fluxes['FUV'] = 1e20*aperture_photometry(astrosat,aperture)['aperture_sum']\n",
    "fluxes['HA'] = aperture_photometry(Halpha,aperture)['aperture_sum']\n",
    "\n",
    "\n",
    "\n",
    "# because the HII-regions are sometimes extended and not circular, this is probably not sufficient\n",
    "'''\n",
    "r_in,r_out = 5*u.arcsec,8*u.arcsec\n",
    "A_circle  = np.pi*aperture_size**2\n",
    "A_annulus = np.pi*(r_out**2-r_in**2)\n",
    "annulus_aperture = SkyCircularAnnulus(positions,r_in=r_in, r_out=r_out)\n",
    "\n",
    "\n",
    "fluxes['FUV_bkg'] = 1e20*aperture_photometry(astrosat,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['HA_bkg'] = aperture_photometry(Halpha,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['FUV'] = fluxes['FUV']-fluxes['FUV_bkg']\n",
    "fluxes['HA']  = fluxes['HA']-fluxes['HA_bkg']\n",
    "'''\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. FUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=1481*u.angstrom)\n",
    "fluxes['FUV'] = fluxes['FUV'] / extinction_mw \n",
    "fluxes['FUV_CORR'] = fluxes['FUV'] / ext_int \n",
    "\n",
    "# the Halpha line maps are already MW extinction corrected\n",
    "ext_int,ext_int_err = extinction(2*associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=6562*u.angstrom)\n",
    "fluxes['HA_CORR'] = fluxes['HA'] / ext_int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA']/fluxes['FUV'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA']/fluxes['FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,125])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,70])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From mask\n",
    "\n",
    "because the resolution of MUSE and astrosat is so much worse than HST, many associations won't be resolved and hence we can not measure the fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "associations_muse, _  = reproject_interp(associations_mask,output_projection=Halpha.wcs,shape_out=Halpha.data.shape,order='nearest-neighbor') \n",
    "associations_astro, _ = reproject_interp(associations_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(set(np.unique(associations_muse[~np.isnan(associations_muse)])) & set(np.unique(associations_astro[~np.isnan(associations_astro)])))\n",
    "sample.sort()\n",
    "HA_flux = [np.sum(Halpha.data[associations_muse==cluster_ID]) for cluster_ID in sample]\n",
    "FUV_flux = [np.sum(astrosat.data[associations_astro==cluster_ID]) for cluster_ID in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "fluxes = Table([sample,HA_flux,FUV_flux],names=['cluster_ID','HA','FUV'])\n",
    "catalogue = join(associations,fluxes[(~np.isnan(HA_flux)) & (~np.isnan(FUV_flux))],keys='cluster_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "xlim=[0,100]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['HA']/catalogue['FUV'])\n",
    "\n",
    "ax.set(xlim=xlim,xlabel='age/Myr',ylabel='Ha/FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem with the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp, reproject_exact\n",
    "from astropy.nddata import block_replicate\n",
    "\n",
    "nebulae_astrosat, _ = reproject_interp(nebulae_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor') \n",
    "astro_MUSE, _ = reproject_exact(astrosat,output_projection=Halpha.wcs,shape_out=Halpha.data.shape)    \n",
    "asttro_fine = block_replicate(astrosat,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ID = 13\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "size = 4*u.arcsecond\n",
    "\n",
    "nebulae_cutout       = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "astrosat_cutout_MUSE = Cutout2D(astro_MUSE,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "nebulae_cutout_astrosat = Cutout2D(nebulae_astrosat,position,size=size,wcs=astrosat.wcs)\n",
    "astrosat_cutout = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "\n",
    "astrosat_up = block_replicate(astrosat_cutout.data,4)\n",
    "nebulae_fine, _ = reproject_interp(nebulae_mask,output_projection=astrosat_cutout.wcs,shape_out=astrosat_up.shape,order='nearest-neighbor') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae_cutout.data[~np.isnan(nebulae_cutout.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout.data)\n",
    "    blank_mask[nebulae_cutout.data==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "neb_contours_astrosat = []\n",
    "for i in np.unique(nebulae_cutout_astrosat.data[~np.isnan(nebulae_cutout_astrosat.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout_astrosat.data)\n",
    "    blank_mask[nebulae_cutout_astrosat.data==i] = 1\n",
    "    neb_contours_astrosat += find_contours(blank_mask, 0.5)\n",
    "\n",
    "neb_contours_fine = []\n",
    "for i in np.unique(nebulae_fine[~np.isnan(nebulae_fine)]):\n",
    "    blank_mask = np.zeros_like(nebulae_fine)\n",
    "    blank_mask[nebulae_fine==i] = 1\n",
    "    neb_contours_fine += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "norm1 = simple_norm(astrosat_cutout.data,clip=False,percent=99)\n",
    "ax1.imshow(astrosat_cutout.data,norm=norm1)\n",
    "\n",
    "norm2 = simple_norm(astrosat_cutout_MUSE.data,clip=False,percent=99)\n",
    "ax2.imshow(astrosat_cutout_MUSE.data,norm=norm2)\n",
    "\n",
    "norm3 = simple_norm(astrosat_up,clip=False,percent=99)\n",
    "ax3.imshow(astrosat_up,norm=norm3)\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.8)\n",
    "for coords in neb_contours_astrosat:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)   \n",
    "for coords in neb_contours_fine:\n",
    "    ax3.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)  \n",
    "    \n",
    "ax1.set_title('original astrosat resolution')\n",
    "ax2.set_title('interpolated to MUSE resolution')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'interpolate regions: {np.sum(astrosat_cutout.data[nebulae_cutout_astrosat.data==region_ID]):.2g}')\n",
    "print(f'interpolate astrosat: {np.sum(np.sum(astrosat_cutout_MUSE.data[nebulae_cutout.data==region_ID])):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_exact\n",
    "\n",
    "ast_small , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(101,101)) \n",
    "ast_large , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(202,202)) \n",
    "ast_org = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "ax1.imshow(ast_org.data)\n",
    "ax2.imshow(ast_small)\n",
    "ax3.imshow(ast_large)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "\n",
    "astrosat_upsampled = block_replicate(astrosat,4,conserve_sum=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with SIGNALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name!='NGC0628':\n",
    "    raise ValueError(f'current Galaxy is {name} and not NGC0628')\n",
    "\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "\n",
    "with open(basedir/'data'/'external'/'columns.txt') as f:\n",
    "    txt = f.read()\n",
    "names = txt.split('\\n')\n",
    "\n",
    "with fits.open(basedir/'data'/'external'/'NGC628_catalog.fits') as hdul:\n",
    "    signals = Table(hdul[0].data,names=names)\n",
    "signals['SkyCoord'] = SkyCoord(signals['RA']*u.degree,signals['DEC']*u.degree)\n",
    "\n",
    "signals['in_frame'] = reg_muse_sky.contains(signals['SkyCoord'],nebulae_mask.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratios(tbl):\n",
    "    '''\n",
    "    'log(NII6583/HA)',\n",
    "    'log(SII6716+6731/HA)',\n",
    "    'log(SII6716+6731/NII6583)',\n",
    "    'log(OIII5007/HB)',\n",
    "    'log(OII3727/HB)',\n",
    "    'log(OII3727+OII5007/HB)',\n",
    "    'log(OIII5007/OII3727 )',\n",
    "    'log(OIII5007/NII6583 )',\n",
    "    'log(OII3727/NII6583)',\n",
    "    'log(SII6717/SII6731)'\n",
    "    ''' \n",
    "    \n",
    "    with np.errstate(divide='ignore'):\n",
    "        tbl['NII6583/HA'] = np.log10(tbl['NII6583_FLUX']/tbl['HA6562_FLUX'])\n",
    "        tbl['SII6716+6731/HA'] = np.log10((tbl['SII6716_FLUX']+tbl['SII6730_FLUX'])/tbl['HA6562_FLUX'])\n",
    "        tbl['SII6716+6731/NII6583'] = np.log10((tbl['SII6716_FLUX']+tbl['SII6730_FLUX'])/tbl['NII6583_FLUX'])\n",
    "        tbl['OIII5007/HB'] = np.log10(tbl['OIII5006_FLUX']/tbl['HB4861_FLUX'])\n",
    "        tbl['OIII5007/NII6583'] = np.log10(tbl['OIII5006_FLUX']/tbl['NII6583_FLUX'])\n",
    "        tbl['SII6717/SII6731'] = np.log10(tbl['SII6716_FLUX']/tbl['SII6730_FLUX'])    \n",
    "\n",
    "    return tbl\n",
    "\n",
    "HII_regions = calculate_ratios(HII_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, Angle\n",
    "\n",
    "matchcoord = HII_regions[HII_regions['flag_point_source']==1]\n",
    "catalogcoord = signals[signals['in_frame']]\n",
    "\n",
    "idx,sep,_=match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "crit = sep.__lt__(Angle('0.8\"'))\n",
    "\n",
    "lines = ['log(NII6583/HA)','log(SII6716+6731/HA)','log(SII6716+6731/NII6583)',\n",
    "             'log(OIII5007/HB)','log(OIII5007/NII6583)','log(SII6717/SII6731)']\n",
    "\n",
    "for line in lines:\n",
    "    matchcoord[line] = catalogcoord[idx][line]\n",
    "matchcoord = matchcoord[crit]\n",
    "\n",
    "#print(f'SIGNALS in MUSE frame: {np.sum(catalogcoord[\"in_frame\"])}')\n",
    "print(f'matches {np.sum(crit)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from region import Regions\n",
    "        \n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(projection=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,stretch='asinh',percent=90)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Greys)\n",
    "\n",
    "x,y = catalogcoord[idx]['SkyCoord'].to_pixel(Halpha.wcs)\n",
    "plt.scatter(x,y,marker='o',fc='none',ec='tab:red',s=2,lw=0.1)\n",
    "\n",
    "for coords in muse_regions.contours: \n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.2)\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/'signals_nebulae.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=2,ncols=3,figsize=(10,6))\n",
    "\n",
    "axes_iter=iter(axes.flatten())\n",
    "\n",
    "for line in lines:\n",
    "    ax = next(axes_iter)\n",
    "    \n",
    "    ax.scatter(matchcoord[line],matchcoord[line[4:-1]])\n",
    "    #ax.plot([-3,3],[-3,3])\n",
    "    ax.set(xlabel=f'{line} SIGNALS',ylabel=f'{line} PHANGS')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starburst99\n",
    "\n",
    "compare our observations with simulated data\n",
    "\n",
    "**Note**: the GENEVAHIGH 23 (Z=0.008) model used a metallicity of 0.02 for the high resolution models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst.core import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAHIGH',metallicity=0.014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax1.plot(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "sc = ax1.scatter(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "                 c=cluster.ewidth['Time']/1e6,vmin=0,vmax=10)\n",
    "fig.colorbar(sc,ax=ax1,label='age / Myr')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='eq width')\n",
    "#plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "#sc = ax1.scatter(FUV_int,Halpha,c=time_HA/1e6,vmin=0,vmax=3)\n",
    "#fig.colorbar(sc,ax=ax1,label='age / Myr')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter response curve\n",
    "\n",
    "to get the FUV flux by integrating the spectrum. The curves are from the [astrosat website](https://uvit.iiap.res.in/Instrument/Filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Halpha and FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "FUV  = cluster.FUV['FUV']\n",
    "time_FUV = cluster.FUV['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha,color='tab:red')\n",
    "ax1.set_ylabel('Halpha/ (erg/s)',color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "FUV_int = np.interp(time_HA,time_FUV,FUV)\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='tab:blue')\n",
    "ax1.set_ylabel('Halpha / FUV',color='tab:blue')\n",
    "ax1.set(xlim=[0,3],ylim=[1e-5,0.0017],xlabel='Time/Myr')\n",
    "\n",
    "axt = ax1.twinx()\n",
    "quanta = cluster.quanta\n",
    "axt.plot(quanta['Time']/1e6,quanta['HI_rate'],color='tab:orange')\n",
    "axt.set_ylabel('ionizing photons / 1/s',color='tab:orange')\n",
    "axt.set(xlim=[0,10])\n",
    "\n",
    "HI_rate_int = np.interp(time_HA,quanta['Time'],quanta['HI_rate'])\n",
    "\n",
    "ax2.plot(HI_rate_int,Ha/FUV_int,color='black')\n",
    "sc = ax2.scatter(HI_rate_int,Ha/FUV_int,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(sc,cax=cax,label='age / Myr',pad=-1)\n",
    "\n",
    "ax2.set(xlabel='ionizing photons / 1/s',ylabel='Halpha / FUV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='black')\n",
    "#sc = ax1.scatter(FUV_int,Halpha,c=time_HA/1e6,vmin=0,vmax=3)\n",
    "#fig.colorbar(sc,ax=ax1,label='age / Myr')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [5e2,1e3,2e3,5e3,1e4,2e4,5e4]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    clusters[m] = Cluster(stellar_model=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "for m in [23,53,63]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax1.legend()\n",
    "\n",
    "for m in [24,54,64]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "\n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax2.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax2.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax2.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    ionizing = cl.quanta['HI_rate']\n",
    "  \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(Halpha/FUV,ionizing,color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(Halpha/FUV,ionizing,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / FUV',xlabel='ionization')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare observations to simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['distance'] = np.nan\n",
    "for name in np.unique(catalogue['gal_name']):\n",
    "    catalogue['distance'][catalogue['gal_name']==name] = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [2e3,5e3,1e4,2e4,5e4,1e5,2e5,5e5,1e6,2e6,5e6]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "Halpha_FLUX = ((catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "FUV_FLUX = 5e5*((catalogue['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "ax1.scatter(np.log10(FUV_FLUX),np.log10(Halpha_FLUX))\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the grid to compare the observations to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = len(cluster.ewidth['Time'])\n",
    "n_mass = 1000\n",
    "\n",
    "mass_min = 5e3\n",
    "mass_max = 1e6\n",
    "\n",
    "mass_grid = np.linspace(mass_min,mass_max,n_mass)\n",
    "\n",
    "HA_grid = np.zeros((n_time,n_mass))\n",
    "FUV_grid = np.zeros((n_time,n_mass))\n",
    "\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    HA_grid[:,i]  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    FUV_grid[:,i] = scaled_cluster.FUV['FUV']\n",
    "\n",
    "time = scaled_cluster.FUV['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)\n",
    "\n",
    "mass, age, chi2 = [],[],[]\n",
    "for row in catalogue:\n",
    "    \n",
    "    Halpha_FLUX = ((row['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    Halpha_ERR  = ((row['HA6562_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    FUV_FLUX = 1e6*((row['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    FUV_ERR  = ((row['FUV_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    chi2_grid = (Halpha_FLUX-HA_grid)**2/Halpha_ERR**2 + (FUV_FLUX-FUV_grid)**2/FUV_ERR**2\n",
    "    \n",
    "    row,col = np.unravel_index(chi2_grid.argmin(), chi2_grid.shape)\n",
    "    mass.append(mass_grid[col])\n",
    "    age.append(time[row].value)\n",
    "    chi2.append(np.min(chi2_grid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,catalogue['AGE_MINCHISQ'])\n",
    "ax1.set(ylim=[0,30],xlim=[0,30],xlabel='age from Nebulae / Myr',ylabel='age from Cluster / Myr')\n",
    "\n",
    "ax2.scatter(np.array(mass),catalogue['MASS_MINCHISQ'])\n",
    "ax2.set(xlabel='mass from Nebulae / Msun',ylabel='mass from Cluster / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(nrows=1,ncols=1,figsize=(single_column,single_column))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,np.array(mass))\n",
    "ax1.set(xlim=[0,30],ylim=[0,5e5],xlabel='age from Nebulae / Myr',ylabel='mass / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "eqwHA  = cluster.ewidth['Equ_width_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,eqwHA,color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr',ylabel='eq / AA',xlim=[0,10])\n",
    "\n",
    "#ax2 = ax1.twinx() \n",
    "#ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "#ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "#ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    eqwHA  = cl.ewidth['Equ_width_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    #sl = cl.scale(1e5)\n",
    "    #eqwHA  = sl.ewidth['Equ_width_H_A']\n",
    "    #time_HA = sl.ewidth['Time']\n",
    "    #ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),ls='--',color=c)\n",
    "    \n",
    "    \n",
    "ax1.set(xlabel='log (Time / Myr)',ylabel=r'log (W(H$\\alpha$) / $\\AA$)',xlim=[6,7.5])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    HAFUV  = cl.ewidth['Luminosity_H_A'] / cl.FUV['FUV']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    sl = cl.scale(1e5)\n",
    "    HAFUV  = sl.ewidth['Luminosity_H_A'] / sl.FUV['FUV']\n",
    "    time_HA = sl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,ls='--',color=c)    \n",
    "    \n",
    "ax1.set(xlabel='Time / Myr',ylabel=r'HA/FUV',xlim=[0,10])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(folder,**kwargs):\n",
    "\n",
    "    parameters = {\n",
    "    \"name\" : 'standard',\n",
    "    \"isf\" : -1,\n",
    "    \"mass\" : 1.,\n",
    "    \"sfr\" : 1,\n",
    "    \"ninterv\" : 2,\n",
    "    \"xponent\" : '1.3,2.3',\n",
    "    \"xmaslim\" : '0.1,0.5,120',\n",
    "    \"sncut\" : 8.,\n",
    "    \"bhcut\" : 120.,\n",
    "    \"model\" : 64,\n",
    "    \"wind_model\" : 0,\n",
    "    \"tinitial\" : 0.01,\n",
    "    \"time_scale\" : 0,\n",
    "    \"time_step\" : 0.1,\n",
    "    \"n_steps\" : 1000,\n",
    "    \"tmax\" : 50,\n",
    "    \"jmg\" : 3,\n",
    "    \"atmos\" : 5,\n",
    "    \"metallicity\" : 3,\n",
    "    \"uvline\" : 1}\n",
    "\n",
    "    # assign the new parameters\n",
    "    for k,v in kwargs.items():\n",
    "        if k in parameters:\n",
    "            parameters[k] = v\n",
    "    \n",
    "    # open tempalte\n",
    "    with open(basedir/'data'/'input.template') as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    template = template.format(**parameters)\n",
    "\n",
    "    #write to templae\n",
    "    with open(folder/'input.out','w') as f:\n",
    "        f.write(template)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age','mass','HA/FUV','HA6562_FLUX','region_area']\n",
    "\n",
    "\n",
    "data = np.zeros((len(catalogue),len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    data[:,i] = catalogue[col].data\n",
    "data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nebulae = PCA(n_components=2)\n",
    "principalComponents = pca_nebulae.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "237.1px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
