{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Single <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues. This notebook is for use with a single galaxy at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pnlf.packages import *\n",
    "\n",
    "from pnlf.constants import tab10, single_column, two_column\n",
    "from cluster.plot import quick_plot, add_scale\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,datefmt='%H:%M:%S',level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "basedir = Path('..')  # where we save stuff (and )\n",
    "data_ext = Path('a:') # raw data\n",
    "\n",
    "# we use the sample table for basic galaxy properties\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "the galaxies listed in `hst_sample` have a cluster catalogue. The galaxies listed in `muse_sample` have astrosat observations to measure the FUV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_sample      = set(['NGC0628','NGC1365', 'NGC1433', 'NGC1566', 'NGC3351', 'NGC3627', 'NGC4535'])\n",
    "astrosat_sample = set([x.stem.split('_')[0] for x in (data_ext/'Astrosat').iterdir() if x.is_file() and x.suffix=='.fits'])\n",
    "muse_sample     = set(sample_table['name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample\n",
    "\n",
    "name = 'NGC1365'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE (DAP + nebulae catalogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import filter_table\n",
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "# DAP linemaps (Halpha and OIII)\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                    meta=hdul['OIII5006_FLUX'].header,\n",
    "                    wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "path = data_ext / 'MUSE_DR2.1' / 'filterImages' \n",
    "sdss_g, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_g_WCS_Pall_mad.fits',header=True)\n",
    "sdss_r, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_r_WCS_Pall_mad.fits',header=True)\n",
    "sdss_i, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_i_WCS_Pall_mad.fits',header=True)\n",
    "    \n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "filename = basedir / 'data' / 'interim' / 'Nebulae_Catalogue_with_FUV_eq_v2p1.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg)\n",
    "nebulae.rename_columns(['cen_x','cen_y'],['x','y'])\n",
    "nebulae.add_index('region_ID')\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "    nebulae[SII>0]['[SIII]/[SII]'] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2+(nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2)\n",
    "\n",
    "nebulae['HIIregion'] = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "HII_regions = filter_table(nebulae,gal_name=name,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "nebulae = filter_table(nebulae,gal_name=name)\n",
    "\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'Nebulae catalogue' /'spatial_masks'/f'{name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "# WFI image (larger FOV)\n",
    "filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    WFI = NDData(data=hdul[0].data,\n",
    "                 meta=hdul[0].header,\n",
    "                 wcs=WCS(hdul[0].header))\n",
    "    \n",
    "# most of the time we do not need the datacubes\n",
    "if False:\n",
    "    #from spectral_cube import SpectralCube\n",
    "    filename = Path('g:') /'Archive'/'MUSE'/'DR2.1'/'datacubes'/f'{name}_DATACUBE_FINAL_WCS_Pall_mad.fits'\n",
    "    with fits.open(filename , memmap=True, mode='denywrite') as hdul:\n",
    "        data_cube   = hdul[1].data\n",
    "        cube_header = hdul[1].header   \n",
    "    \n",
    "print(f'{name}: {len(HII_regions)} HII-regions in final catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HST\n",
    "\n",
    "**white light + filter images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "target  = name.lower()\n",
    "scalepc = 32\n",
    "\n",
    "# whitelight image (we set 0s to nan)\n",
    "with fits.open(data_ext / 'HST' / 'white_light' / f'{name.lower()}_white_24rgb.fits') as hdul:\n",
    "    hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "    \n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_exp_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_err_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275.uncertainty = StdDevUncertainty(hdul[0].data)\n",
    "    \n",
    "associations, associations_mask = read_associations(folder=data_ext/'HST',target=target,scalepc=scalepc)\n",
    "\n",
    "# modify table (rename the columns such that the clusters and associations are identical)\n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)\n",
    "associations.rename_columns(['reg_id','reg_ra','reg_dec','reg_x','reg_y',\n",
    "                             'reg_dolflux_Age_MinChiSq','reg_dolflux_Mass_MinChiSq','reg_dolflux_Ebv_MinChiSq',\n",
    "                             'reg_dolflux_Age_MinChiSq_err','reg_dolflux_Mass_MinChiSq_err','reg_dolflux_Ebv_MinChiSq_err'],\n",
    "                            ['assoc_ID','RA','DEC','X','Y','age','mass','EBV','age_err','mass_err','EBV_err'])\n",
    "for col in list(associations.columns):\n",
    "    if col.endswith('mjy'):\n",
    "        associations[f'{col.split(\"_\")[0]}_FLUX'] = 1e20*associations[col]*u.mJy.to(u.erg/u.s/u.cm**2/u.Hz)\n",
    "    if col.endswith('mjy_err'):\n",
    "        associations[f'{col.split(\"_\")[0]}_FLUX_ERR'] = 1e20*associations[col]*u.mJy.to(u.erg/u.s/u.cm**2/u.Hz)\n",
    "\n",
    "print(f'{name}: {len(associations)} associations in catalogue')    \n",
    "# associations mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Astrosat\n",
    "\n",
    "https://uvit.iiap.res.in/Instrument/Filters\n",
    "\n",
    "the resolution is 0.4\" per pixel. With a PSF resolution of 1.8\" this leads to fwhm ~ 4.5 px. This corresponds to a std = 1.91 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {name}')\n",
    "    \n",
    "with fits.open(astro_file) as hdul:\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Equivalent Width\n",
    "\n",
    "the first step is to extract the spectra of each HII-region.\n",
    "\n",
    "Are the spectra continuum subtracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'Nebulae catalogue' /'spectra'/f'{name}_VorSpectra.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    spectra = Table(hdul[1].data)\n",
    "    spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    \n",
    "spectra['region_ID'] = np.arange(len(spectra))\n",
    "spectra.add_index('region_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H0 = 67 * u.km / u.s / u.Mpc\n",
    "z = (H0*Distance(distmod=p['(m-M)'])/c.c).decompose()\n",
    "lam_HA0 = 6562.8*u.Angstrom\n",
    "lam_HA = (1+z)*lam_HA0\n",
    "\n",
    "lam_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.spectrum import fit_emission_line\n",
    "\n",
    "region_ID = 10\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "filename = basedir/'reports'/name/f'{name}_eqwidth.png'\n",
    "flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "fit = fit_emission_line(spectral_axis,flux,lam_HA,filename=filename)\n",
    "integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "\n",
    "print(f\"f_cat/f_fit = {nebulae.loc[region_ID]['HA6562_FLUX']/integrated_flux.value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "\n",
    "HA = []\n",
    "HII_regions['eq_width'] = np.nan\n",
    "for region_ID in tqdm(HII_regions['region_ID']):\n",
    "    \n",
    "    flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "    fit = fit_emission_line(spectral_axis,flux,lam_HA,plot=False)\n",
    "    integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "    continuum = fit.c0_1 * u.erg/u.s/u.cm**2/u.Angstrom\n",
    "    eq_width = integrated_flux/continuum\n",
    "    eq_width = HII_regions.loc[region_ID]['HA6562_FLUX']/continuum\n",
    "    HII_regions.loc[region_ID]['eq_width'] = eq_width.value\n",
    "    \n",
    "    HA.append(integrated_flux)\n",
    "    #HA_cat = nebulae.loc[region_ID]['HA6562_FLUX']\n",
    "    #print(f'{integrated_flux/HA_cat:.2f}')\n",
    "    #print(f'HA = {fit.mean_0.value:.2f}')\n",
    "HA = np.array([x.value for x in HA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(HA,HII_regions['HA6562_FLUX']/2.5)\n",
    "x = np.linspace(0,2e6)\n",
    "ax.plot(x,x,color='black')\n",
    "ax.set(ylim=[0,2e6],xlim=[0,2e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HST and MUSE\n",
    "\n",
    "### compare footprints of different observations\n",
    "\n",
    "here we compare the footprints of the observations and check which objects overlap. The FOV of astrosat is a circle with a diameter of 28'. This is much larger than HST and MUSE and both will always be covered by the astrosat observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "\n",
    "# check which nebulae/clusters are within the HST/MUSE FOV\n",
    "associations['in_frame'] = reg_muse_sky.contains(associations['SkyCoord'],nebulae_mask.wcs)\n",
    "#clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "nebulae['in_frame']  = reg_hst_sky.contains(nebulae['SkyCoord'],nebulae_mask.wcs)\n",
    "\n",
    "print(f'{np.sum(associations[\"in_frame\"])} (of {len(associations)}) associations in MUSE FOV')\n",
    "#print(f'{np.sum(clusters[\"in_frame\"])} (of {len(clusters)}) clusters in MUSE FOV')\n",
    "print(f'{np.sum(nebulae[\"in_frame\"])} (of {len(nebulae)}) nebulae in HST FOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=6*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "ax = quick_plot(WFI_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/'footpring.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association and nebulae\n",
    "\n",
    "the association catalogue differs from the clusters in that its entries are extended. Because we match two catalogues with extended objects, we must proceed differently.\n",
    "\n",
    "In a first step we take a look at a single association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "assoc_ID = 10\n",
    "pos = associations['SkyCoord'][associations['assoc_ID']==assoc_ID]\n",
    "\n",
    "contours = find_contours(associations_mask.data==assoc_ID,0.5,)\n",
    "coords = max(contours,key=len)\n",
    "\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_pix  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_sky  = reg_pix.to_sky(associations_mask.wcs)\n",
    "\n",
    "mask_cutout = Cutout2D(associations_mask.data,pos,size=1*u.arcsecond,wcs=associations_mask.wcs)\n",
    "F275_cutout = Cutout2D(F275.data,pos,size=2*u.arcsecond,wcs=F275.wcs)\n",
    "\n",
    "reg_pix_cut  = reg_sky.to_pixel(mask_cutout.wcs)\n",
    "\n",
    "ax = quick_plot(F275_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "ax.imshow(mask_cutout.data,alpha=0.5)\n",
    "\n",
    "reg_pix_cut.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to match the nebulae to the associations we first reproject the mask of the nebulae to the HST image. We then scale the association mask by the number of associations (assume we have 1432 objects, then 615 becomes 0.0615). This way we can add the two masks together and infer from the resulting unique values which clusters overlap with which associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match catalogues\n",
    "\n",
    "and make some plots that showcase the position/overlap\n",
    "\n",
    "```\n",
    "# NGC3627 is too large to reproject\n",
    "#center = SkyCoord(sample_table.loc[name]['R.A.'],sample_table.loc[name]['Dec.'])\n",
    "#cutout = Cutout2D(associations_mask.data,center,size=(5.5*u.arcmin,3*u.arcmin),wcs=associations_mask.wcs)\n",
    "cutout = Cutout2D(associations_mask.data,(7000,7000),size=(9000,7000),wcs=associations_mask.wcs)\n",
    "\n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=cutout.wcs,\n",
    "                                   shape_out=cutout.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "scale = 10**np.ceil(np.log10(max(cutout.data[~np.isnan(cutout.data)])))\n",
    "s_arr = cutout.data/scale+nebulae_hst\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "# reproject nebulae mask to hst \n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=associations_mask.wcs,\n",
    "                                   shape_out=associations_mask.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "\n",
    "# we scale the associations such that the the id is in the decimal\n",
    "scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "s_arr = associations_mask.data/scale+nebulae_hst\n",
    "\n",
    "header = associations_mask.wcs.to_header()\n",
    "header['scale'] = scale\n",
    "hdu = fits.PrimaryHDU(s_arr,header=header)\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_map.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids of associations, nebulae and combination (sum) of both\n",
    "a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "n_id = np.unique(nebulae_hst[~np.isnan(nebulae_hst)]).astype(int)\n",
    "s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "# this splits the sum into two parts (nebulae and associations)\n",
    "a_modf,n_modf = np.modf(s_id)\n",
    "n_modf = n_modf.astype(int)\n",
    "a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "nebulae_dict = {int(n) : a_modf[n_modf==n].tolist() for n in n_id}     \n",
    "associations_dict = {int(a) : n_modf[a_modf==a].tolist() for a in a_id}     \n",
    "\n",
    "\n",
    "# so far we ensured that the nebulae in unique_n have only one association,\n",
    "# but it is possible that this association goes beyond the nebulae and into\n",
    "# a second nebulae. Those objects are excluded here\n",
    "isolated_nebulae = set()\n",
    "isolated_assoc   = set()\n",
    "for n,v in nebulae_dict.items():\n",
    "    if len(v)==1:\n",
    "        if len(associations_dict[v[0]])==1:\n",
    "            isolated_nebulae.add(n)\n",
    "            isolated_assoc.add(v[0])\n",
    "            \n",
    "print(f'n_associations = {len(associations_dict)}')\n",
    "print(f'n_nebulae      = {len(nebulae_dict)}')\n",
    "print(f'1to1 match     = {len(isolated_nebulae)}')\n",
    "\n",
    "\n",
    "# we save those two dicts so we do not have to redo this everytime\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_nebulae.yml','w+') as f:\n",
    "    yaml.dump(nebulae_dict,f)\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_associations.yml','w+') as f:\n",
    "    yaml.dump(associations_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all assoc that have at least one pixel outside of the nebulae masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[~np.isnan(nebulae_hst)] = np.nan\n",
    "outside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "# find all assoc that have at least one pixel inside of the nebulea masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[np.isnan(nebulae_hst)] = np.nan\n",
    "inside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "contained = np.setdiff1d(inside,outside)\n",
    "partial   = np.intersect1d(inside,outside)\n",
    "isolated  = np.setdiff1d(outside,inside)\n",
    "\n",
    "print(f'contained: {len(contained)}\\npartial: {len(partial)}\\nisolated: {len(isolated)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_tmp = associations[['assoc_ID']].copy()\n",
    "assoc_tmp.add_index('assoc_ID')\n",
    "\n",
    "assoc_tmp['overlap'] = np.empty(len(associations),dtype='U9')\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],contained)] = 'contained'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],partial)]   = 'partial'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],isolated)]  = 'isolated'\n",
    "assoc_tmp['1to1'] = False\n",
    "assoc_tmp['1to1'][np.isin(assoc_tmp['assoc_ID'],list(isolated_assoc))] = True\n",
    "assoc_tmp['Nnebulae'] = [len(associations_dict[k]) for k in assoc_tmp['assoc_ID']]\n",
    "\n",
    "assoc_tmp['region_ID'] = np.nan\n",
    "assoc_tmp['region_ID'][assoc_tmp['1to1']] = [associations_dict[k][0] for k in assoc_tmp[assoc_tmp['1to1']]['assoc_ID']]\n",
    "\n",
    "hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_associations.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_neighbors\n",
    "from tqdm import tqdm \n",
    "\n",
    "nebulae_tmp = nebulae[['region_ID','x','y']].copy()\n",
    "nebulae_tmp.add_index('region_ID')\n",
    "\n",
    "nebulae_tmp['neighbors'] = np.nan\n",
    "for row in tqdm(nebulae_tmp):\n",
    "    row['neighbors'] = len(find_neighbors(nebulae_mask.data,tuple(row[['x','y']]),row['region_ID'],plot=False))\n",
    "del nebulae_tmp[['x','y']]\n",
    "\n",
    "nebulae_tmp['1to1'] = False\n",
    "nebulae_tmp['1to1'][np.isin(nebulae_tmp['region_ID'],list(isolated_nebulae))] = True\n",
    "nebulae_tmp['Nassoc'] = [len(nebulae_dict[k]) for k in nebulae_tmp['region_ID']]\n",
    "nebulae_tmp['assoc_ID'] = np.nan\n",
    "nebulae_tmp['assoc_ID'][nebulae_tmp['1to1']] = [nebulae_dict[k][0] for k in nebulae_tmp[nebulae_tmp['1to1']]['region_ID']]\n",
    "    \n",
    "hdu = fits.BinTableHDU(nebulae_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_nebulae.fits',overwrite=True)\n",
    "#del nebulae_tmp['1to1']\n",
    "\n",
    "print(f'{np.sum(nebulae_tmp[\"neighbors\"]==0)} nebulae have no neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "catalogue = join(assoc_tmp,nebulae_tmp,keys=['assoc_ID','region_ID'])\n",
    "catalogue = join(catalogue,nebulae,keys='region_ID')\n",
    "catalogue = join(catalogue,associations,keys='assoc_ID')\n",
    "\n",
    "catalogue.rename_columns(['X','Y','x','y','RA','DEC','cen_ra','cen_dec','reg_area','region_area',\n",
    "                          'EBV_1','EBV_2','EBV_err','EBV_ERR','SkyCoord_1','SkyCoord_2'],\n",
    "                         ['x_asc','y_asc','x_neb','y_neb','ra_asc','dec_asc','ra_neb','dec_neb',\n",
    "                          'area_asc','area_neb','EBV_stars','EBV_balmer','EBV_stars_err','EBV_balmer_err',\n",
    "                          'SkyCoord_asc','SkyCoord_neb'])\n",
    "\n",
    "# separation to other associations and nebulae\n",
    "idx,sep_asc,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_neb,_= match_coordinates_sky(catalogue['SkyCoord_neb'],nebulae['SkyCoord'],nthneighbor=2)\n",
    "catalogue['sep_asc'] = sep_asc.to(u.arcsec)\n",
    "catalogue['sep_neb'] = sep_neb.to(u.arcsec)\n",
    "\n",
    "# select the columns of the joined catalogue\n",
    "columns = ['assoc_ID','region_ID','x_asc','y_asc','x_neb','y_neb',\n",
    "           'ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_asc','SkyCoord_neb','area_asc','area_neb',\n",
    "           'sep_asc','sep_neb','neighbors','overlap',\n",
    "           'age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','EBV_balmer','EBV_balmer_err',\n",
    "           'met_scal','met_scal_err','logq_D91','logq_D91_err',] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR')] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR_ERR')] + \\\n",
    "            ['NUV_FLUX','NUV_FLUX_ERR','U_FLUX','U_FLUX_ERR','B_FLUX','B_FLUX_ERR',\n",
    "             'V_FLUX','V_FLUX_ERR','I_FLUX','I_FLUX_ERR'] + \\\n",
    "            ['HA/FUV','eq_width']\n",
    "catalogue = catalogue[columns]\n",
    "        \n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR')],\n",
    "                      [col.replace('FLUX_CORR','flux') for col in catalogue.columns if col.endswith('FLUX_CORR')])\n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')],\n",
    "                      [col.replace('FLUX_CORR_ERR','flux_err') for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')])\n",
    "catalogue['assoc_ID'] = catalogue['assoc_ID'].astype('int')\n",
    "catalogue['region_ID'] = catalogue['region_ID'].astype('int')\n",
    "\n",
    "catalogue.info.description = 'Joined catalogue between associations and nebulae'\n",
    "mean_sep = np.mean(catalogue['SkyCoord_asc'].separation(catalogue['SkyCoord_neb']))\n",
    "print(f'{len(catalogue)} objects in catalogue')\n",
    "print(f'the mean separation between cluster and association center is {mean_sep.to(u.arcsecond):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write\n",
    "export parts of the joined catalogue (right now only the fully contained objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = catalogue.copy() #[catalogue['contained']]\n",
    "#export.add_column(export['SkyCoord_asc'].to_string(style='hmsdms',precision=2),index=6,name='RaDec_asc')\n",
    "#export.add_column(export['SkyCoord_neb'].to_string(style='hmsdms',precision=2),index=8,name='RaDec_neb')\n",
    "\n",
    "RA_asc ,DEC_asc = zip(*[x.split(' ') for x in export['SkyCoord_asc'].to_string(style='hmsdms',precision=2)])\n",
    "RA_neb ,DEC_neb = zip(*[x.split(' ') for x in export['SkyCoord_neb'].to_string(style='hmsdms',precision=2)])\n",
    "\n",
    "export.add_column(RA_asc,index=6,name='Ra_asc')\n",
    "export.add_column(DEC_asc,index=8,name='Dec_asc')\n",
    "export.add_column(RA_neb,index=10,name='Ra_neb')\n",
    "export.add_column(DEC_neb,index=12,name='Dec_neb')\n",
    "\n",
    "for col in export.columns:\n",
    "    if col not in ['Ra_asc','Dec_asc','Ra_neb','Dec_neb','region_ID','cluster_ID','overlap']:\n",
    "        export[col].info.format = '%.2f'\n",
    "\n",
    "del export[['ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_neb','SkyCoord_asc','HA/FUV']]\n",
    "\n",
    "hdu = fits.BinTableHDU(export,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_associations_and_nebulae_joined.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_nebulae.yml') as f:\n",
    "    nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_associations.yml') as f:\n",
    "    associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    \n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_associations.fits'\n",
    "assoc_tmp = Table(fits.getdata(filename,ext=1))\n",
    "associations=join(associations,assoc_tmp,keys='assoc_ID')\n",
    "\n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_nebulae.fits'\n",
    "nebulae_tmp = Table(fits.getdata(filename,ext=1))\n",
    "nebulae=join(nebulae,nebulae_tmp,keys='region_ID')\n",
    "\n",
    "# read in existing catalogues\n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_associations_and_nebulae_joined.fits'\n",
    "catalogue = Table(fits.getdata(filename,ext=1))\n",
    "\n",
    "catalogue['SkyCoord_asc'] = SkyCoord(catalogue['Ra_asc'],catalogue['Dec_asc'])\n",
    "catalogue['SkyCoord_neb'] = SkyCoord(catalogue['Ra_neb'],catalogue['Dec_neb'])\n",
    "catalogue['HA/FUV'] = catalogue['HA6562_flux']/catalogue['FUV_flux']\n",
    "catalogue['HA/FUV_err'] = catalogue['HA/FUV']*np.sqrt((catalogue['HA6562_flux_err']/catalogue['HA6562_flux'])**2+(catalogue['FUV_flux_err']/catalogue['FUV_flux'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### save cutout for each region in seperate fits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_interp \n",
    "\n",
    "def save_cutouts(position,size=4*u.arcsec):\n",
    "    \n",
    "    header = fits.Header()\n",
    "    header['gal_name'] = name\n",
    "    header['RA'] = row['SkyCoord_asc'].ra.to(u.degree).value\n",
    "    header['DEC'] = row['SkyCoord_asc'].dec.to(u.degree).value\n",
    "    header['RADESYS'] = 'ICRS'\n",
    "    header['regionID'] = row['region_ID']\n",
    "    header['assocID'] = row['cluster_ID']\n",
    "\n",
    "    hdul = fits.HDUList([fits.PrimaryHDU(header=header)])\n",
    "\n",
    "    # save HST image\n",
    "    cutout = Cutout2D(F275.data,position=position,size=size,wcs=F275.wcs)\n",
    "    hdul.append(fits.ImageHDU(cutout.data,header=cutout.wcs.to_header(),name='F275'))\n",
    "\n",
    "    # save Halpha\n",
    "    Halpha_cutout, _  = reproject_interp(Halpha,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(Halpha_cutout,header=cutout.wcs.to_header(),name='Halpha'))\n",
    "\n",
    "\n",
    "    # save OIII\n",
    "    OIII_cutout, _  = reproject_interp(OIII,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(OIII_cutout,header=cutout.wcs.to_header(),name='OIII'))\n",
    "\n",
    "\n",
    "    # save nebulae mask\n",
    "    nebulae_cutout, _  = reproject_interp(nebulae_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(nebulae_cutout,header=cutout.wcs.to_header(),name='nebulae'))\n",
    "\n",
    "\n",
    "    # save association mask\n",
    "    assoc_cutout, _  = reproject_interp(associations_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(assoc_cutout,header=cutout.wcs.to_header(),name='assoc'))\n",
    "\n",
    "    hdul.writeto(basedir/'data'/'cutouts'/f'{name}_region{row[\"region_ID\"]}.fits',overwrite=True,checksum=True)\n",
    "    \n",
    "    \n",
    "row = catalogue[0]\n",
    "position = row['SkyCoord_neb']\n",
    "\n",
    "save_cutouts(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cutouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the joined catalogue (containing only nebulae and clusters with a 1 to 1 relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import single_cutout\n",
    "\n",
    "region_ID = 23\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "single_cutout(ax=ax,position = position,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             #label= f'{region_ID}/{nebulae_dict[region_ID][0]}',\n",
    "             size = 4*u.arcsecond)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'single_region.png',dpi=315)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "offset = catalogue['SkyCoord_asc'].separation(catalogue['SkyCoord_neb'])\n",
    "idx,sep,_=match_coordinates_sky(catalogue['SkyCoord_asc'],catalogue['SkyCoord_asc'],nthneighbor=2)\n",
    "\n",
    "\n",
    "sample = catalogue[(catalogue['contained']) & (catalogue['neighbors']==0) & (offset<Angle('0.2\"')) & (sep>Angle('5\"'))]\n",
    "print(f'{len(sample)} objects in catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_cutout\n",
    "\n",
    "#sample = catalogue[:12]\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = sample['SkyCoord_neb']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in sample[['region_ID','cluster_ID']]]\n",
    "\n",
    "multi_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a multi page pdf with all isolated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = catalogue['SkyCoord_neb'][catalogue['overlap']=='contained']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in catalogue[['region_ID','assoc_ID']][catalogue['overlap']=='contained']]\n",
    "\n",
    "multi_page_cutout(positions = positions[:59],\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=4*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column),subplot_kw={'projection': WFI_cutout.wcs})\n",
    "norm = simple_norm(WFI_cutout.data,clip=False,percent=96)\n",
    "ax.imshow(WFI_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "x,y = catalogue['SkyCoord_neb'][catalogue['contained']].to_pixel(WFI_cutout.wcs)\n",
    "ax.scatter(x,y,marker='s',facecolors='none',s=30,lw=1,color='tab:red')\n",
    "\n",
    "ax.coords[0].set_ticks_visible(False)\n",
    "ax.coords[1].set_ticks_visible(False)\n",
    "ax.coords[0].set_ticklabel_visible(False)\n",
    "ax.coords[1].set_ticklabel_visible(False)\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_location_in_galaxy.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in joined catalogue\n",
    "\n",
    "first we create a mask to select a subset of objects (e.g. based on mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "# separation to other associations\n",
    "idx,sep_others,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_int,_= match_coordinates_sky(catalogue['SkyCoord_asc'],catalogue['SkyCoord_neb'])\n",
    "\n",
    "# size of the association compared to the HII-region\n",
    "small_HII = (catalogue['area_neb']/0.039) / (catalogue['area_asc']/11.95) > 2\n",
    "\n",
    "# distance to centre of galaxy\n",
    "galactic_center = SkyCoord(ra=p['R.A.'],dec=p['Dec.'])\n",
    "catalogue['galactic_radius'] = catalogue['SkyCoord_neb'].separation(galactic_center).to(u.arcmin)\n",
    "\n",
    "# define the criteria which objects we use in the plot\n",
    "criteria = (catalogue['mass']>5e3) & catalogue['contained'] #& (catalogue['age']>catalogue['age_err']) \n",
    "\n",
    "#& catalogue['contained'] #& (catalogue['galactic_radius']>1*u.arcmin)\n",
    "\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HA/FUV vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],fmt='o')\n",
    "            #xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'])\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equivalent width vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(tmp['age'],tmp['eq_width']) #,c=tmp['mass'],vmin=1e5,vmax=3e5)\n",
    "#fig.colorbar(sc,label='mass / Msun')\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['eq_width'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='equivalent width / Angstrom',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_eq_width_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,100]\n",
    "criteria = HII_regions['HA/FUV']<150\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria])\n",
    "\n",
    "x,mean,std = bin_stat(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='equivalent width / Angstrom',ylabel='Halpha / FUV',xlim=xlim,ylim=(0,70))\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_eq_width.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extinction from stars and from nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# we expect EBV_balmer = 2 EBV_stars\n",
    "\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=plt.cm.viridis_r)\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_EBV_Balmer_vs_Stars.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Plot\n",
    "\n",
    "are there any obvious differences between eg the old, high EW regions versus young, low EW regions?\n",
    "\n",
    "Likewise for the HA/FUV vs HA EQW plot, how much of this correlation is driven by Halpha, rather than a general trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "catalogue['HA/NUV'] = catalogue['HA6562_flux'] / catalogue['NUV_FLUX'] / 1e12\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_corner'\n",
    "columns  = ['age','HA/FUV','HA/NUV','met_scal','logq_D91']\n",
    "limits   = {'age':(0,10),'eq_width':(0,100),'HA/FUV':(0,50),'HA/NUV':(0,20),'met_scal':(8.4,8.7),'logq_D91':(6,8)}\n",
    "\n",
    "tmp = catalogue[catalogue['contained']]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "corner(tmp,columns,limits,nbins=5,filename=filename,vmin=1000,vmax=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "lines = [col for col in catalogue.columns if col.endswith('_flux')]\n",
    "print(f'{len(lines)} different lines with {comb(len(lines),2)} possible combinations')\n",
    "\n",
    "correlation = []\n",
    "for pair in itertools.combinations(lines,2):\n",
    "    not_nan = ~np.isnan(catalogue[pair[0]]) & ~np.isnan(catalogue[pair[1]])\n",
    "    r,p = spearmanr(catalogue['age'],catalogue[pair[0]][not_nan]/catalogue[pair[1]][not_nan])\n",
    "    correlation.append((r,pair))\n",
    "a = [x for x in correlation if np.abs(x[0])>0.15]\n",
    "a.sort(key=lambda x: np.abs(x[0]),reverse=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "rho,(line1,line2) = a[-1]\n",
    "ax.scatter(catalogue['age'],catalogue[line1]/catalogue[line2])\n",
    "ax.set(xlabel='age',ylabel=f\"{line2.split('_')[0]}/{line1.split('_')[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['HA/SII'] = catalogue['HA6562_flux'] / catalogue['SII6716_flux']\n",
    "catalogue['HA/OI'] = catalogue['HA6562_flux'] / catalogue['OI6300_flux']\n",
    "catalogue['HA/SII'][~np.isfinite(catalogue['HA/SII'])] = np.nan\n",
    "catalogue['HA/OI'][~np.isfinite(catalogue['HA/OI'])] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halpha luminosity vs mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1e4,5e5]\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] \n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['mass'],tmp['HA6562_flux'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['HA6562_flux'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='Halpha',\n",
    "       xlim=xlim,ylim=[1e4,1e6],xscale='log',yscale='log')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1,2e5]\n",
    "criteria = (catalogue['age']<20) #& (sep>Angle('3\"'))\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(tmp['mass'],tmp['region_area'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['region_area'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='HII-region area',xlim=xlim)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associated vs isolated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[tmp['overlap']=='isolated']['age']\n",
    "\n",
    "print(f'ages: con={np.mean(ages_con):.2f}, par={np.mean(ages_par):.2f}, iso={np.mean(ages_iso):.2f}')\n",
    "\n",
    "ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "ax1.set_title(f'contained ({np.mean(ages_con):.2f} Myr)')\n",
    "ax2.set_title(f'partially ({np.mean(ages_par):.2f} Myr)')\n",
    "ax3.set_title(f'isolated ({np.mean(ages_iso):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,120],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_contained.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_sep.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Measure FUV and Halpha at association position\n",
    "\n",
    "### From point\n",
    "\n",
    "If I select an aperture smaller than a pixel, the measured flux is directly proportional to the apertuer size. Therefore it doesn't matter that the astrosat resolution is much worse than HST or MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = CCM89(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import SkyCircularAperture,SkyCircularAnnulus,aperture_photometry\n",
    "\n",
    "criteria = np.isin(associations['cluster_ID'],isolated_assoc)\n",
    "\n",
    "aperture_size = 1*u.arcsecond\n",
    "positions = associations['SkyCoord'][criteria]\n",
    "\n",
    "aperture = SkyCircularAperture(positions,aperture_size)\n",
    "\n",
    "fluxes = associations[['cluster_ID','SkyCoord','age','age_err','mass','mass_err','EBV','EBV_err']][criteria]\n",
    "fluxes['FUV'] = 1e20*aperture_photometry(astrosat,aperture)['aperture_sum']\n",
    "fluxes['HA'] = aperture_photometry(Halpha,aperture)['aperture_sum']\n",
    "\n",
    "\n",
    "\n",
    "# because the HII-regions are sometimes extended and not circular, this is probably not sufficient\n",
    "'''\n",
    "r_in,r_out = 5*u.arcsec,8*u.arcsec\n",
    "A_circle  = np.pi*aperture_size**2\n",
    "A_annulus = np.pi*(r_out**2-r_in**2)\n",
    "annulus_aperture = SkyCircularAnnulus(positions,r_in=r_in, r_out=r_out)\n",
    "\n",
    "\n",
    "fluxes['FUV_bkg'] = 1e20*aperture_photometry(astrosat,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['HA_bkg'] = aperture_photometry(Halpha,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['FUV'] = fluxes['FUV']-fluxes['FUV_bkg']\n",
    "fluxes['HA']  = fluxes['HA']-fluxes['HA_bkg']\n",
    "'''\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. FUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=1481*u.angstrom)\n",
    "fluxes['FUV'] = fluxes['FUV'] / extinction_mw \n",
    "fluxes['FUV_CORR'] = fluxes['FUV'] / ext_int \n",
    "\n",
    "# the Halpha line maps are already MW extinction corrected\n",
    "ext_int,ext_int_err = extinction(2*associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=6562*u.angstrom)\n",
    "fluxes['HA_CORR'] = fluxes['HA'] / ext_int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA']/fluxes['FUV'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA']/fluxes['FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,125])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,70])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### From mask\n",
    "\n",
    "because the resolution of MUSE and astrosat is so much worse than HST, many associations won't be resolved and hence we can not measure the fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "associations_muse, _  = reproject_interp(associations_mask,output_projection=Halpha.wcs,shape_out=Halpha.data.shape,order='nearest-neighbor') \n",
    "associations_astro, _ = reproject_interp(associations_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = list(set(np.unique(associations_muse[~np.isnan(associations_muse)])) & set(np.unique(associations_astro[~np.isnan(associations_astro)])))\n",
    "sample.sort()\n",
    "HA_flux = [np.sum(Halpha.data[associations_muse==cluster_ID]) for cluster_ID in sample]\n",
    "FUV_flux = [np.sum(astrosat.data[associations_astro==cluster_ID]) for cluster_ID in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "fluxes = Table([sample,HA_flux,FUV_flux],names=['cluster_ID','HA','FUV'])\n",
    "catalogue = join(associations,fluxes[(~np.isnan(HA_flux)) & (~np.isnan(FUV_flux))],keys='cluster_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bins = 10\n",
    "xlim=[0,100]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['HA']/catalogue['FUV'])\n",
    "\n",
    "ax.set(xlim=xlim,xlabel='age/Myr',ylabel='Ha/FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### problem with the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_interp, reproject_exact\n",
    "from astropy.nddata import block_replicate\n",
    "\n",
    "nebulae_astrosat, _ = reproject_interp(nebulae_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor') \n",
    "astro_MUSE, _ = reproject_exact(astrosat,output_projection=Halpha.wcs,shape_out=Halpha.data.shape)    \n",
    "asttro_fine = block_replicate(astrosat,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "region_ID = 13\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "size = 4*u.arcsecond\n",
    "\n",
    "nebulae_cutout       = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "astrosat_cutout_MUSE = Cutout2D(astro_MUSE,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "nebulae_cutout_astrosat = Cutout2D(nebulae_astrosat,position,size=size,wcs=astrosat.wcs)\n",
    "astrosat_cutout = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "\n",
    "astrosat_up = block_replicate(astrosat_cutout.data,4)\n",
    "nebulae_fine, _ = reproject_interp(nebulae_mask,output_projection=astrosat_cutout.wcs,shape_out=astrosat_up.shape,order='nearest-neighbor') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae_cutout.data[~np.isnan(nebulae_cutout.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout.data)\n",
    "    blank_mask[nebulae_cutout.data==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "neb_contours_astrosat = []\n",
    "for i in np.unique(nebulae_cutout_astrosat.data[~np.isnan(nebulae_cutout_astrosat.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout_astrosat.data)\n",
    "    blank_mask[nebulae_cutout_astrosat.data==i] = 1\n",
    "    neb_contours_astrosat += find_contours(blank_mask, 0.5)\n",
    "\n",
    "neb_contours_fine = []\n",
    "for i in np.unique(nebulae_fine[~np.isnan(nebulae_fine)]):\n",
    "    blank_mask = np.zeros_like(nebulae_fine)\n",
    "    blank_mask[nebulae_fine==i] = 1\n",
    "    neb_contours_fine += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "norm1 = simple_norm(astrosat_cutout.data,clip=False,percent=99)\n",
    "ax1.imshow(astrosat_cutout.data,norm=norm1)\n",
    "\n",
    "norm2 = simple_norm(astrosat_cutout_MUSE.data,clip=False,percent=99)\n",
    "ax2.imshow(astrosat_cutout_MUSE.data,norm=norm2)\n",
    "\n",
    "norm3 = simple_norm(astrosat_up,clip=False,percent=99)\n",
    "ax3.imshow(astrosat_up,norm=norm3)\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.8)\n",
    "for coords in neb_contours_astrosat:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)   \n",
    "for coords in neb_contours_fine:\n",
    "    ax3.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)  \n",
    "    \n",
    "ax1.set_title('original astrosat resolution')\n",
    "ax2.set_title('interpolated to MUSE resolution')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f'interpolate regions: {np.sum(astrosat_cutout.data[nebulae_cutout_astrosat.data==region_ID]):.2g}')\n",
    "print(f'interpolate astrosat: {np.sum(np.sum(astrosat_cutout_MUSE.data[nebulae_cutout.data==region_ID])):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_exact\n",
    "\n",
    "ast_small , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(101,101)) \n",
    "ast_large , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(202,202)) \n",
    "ast_org = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "ax1.imshow(ast_org.data)\n",
    "ax2.imshow(ast_small)\n",
    "ax3.imshow(ast_large)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "\n",
    "astrosat_upsampled = block_replicate(astrosat,4,conserve_sum=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starburst99\n",
    "\n",
    "compare our observations with simulated data\n",
    "\n",
    "**Note**: the GENEVAHIGH 23 (Z=0.008) model used a metallicity of 0.02 for the high resolution models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multiple Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test what multiple stellar populations would look like\n",
    "\n",
    "dT = 0.5*np.array([1,2,3,4,5]) * u.Myr\n",
    "\n",
    "time = cluster.ewidth['Time']\n",
    "Ha  = cluster.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster.FUV['FUV'].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(1.5*two_column,two_column/2))\n",
    "\n",
    "if False:\n",
    "    ax1.plot(time.value/1e6,Ha,label=f'{0*u.Myr}')\n",
    "    ax2.plot(time.value/1e6,FUV,label=f'{0*u.Myr}')   \n",
    "    ax3.plot(time.value/1e6,Ha/FUV,label=f'{0*u.Myr}')   \n",
    "\n",
    "for t in dT:\n",
    "    c0 = cluster.time_shift(t)\n",
    "    Ha_new  = np.interp(time,c0.ewidth['Time'],c0.ewidth['Luminosity_H_A'],left=0,right=0)\n",
    "    FUV_new = np.interp(time,c0.FUV['Time'],c0.FUV['FUV'],left=0,right=0)   \n",
    "     \n",
    "    ax1.axvline(t.value,color='black',ls='--')\n",
    "    ax2.axvline(t.value,color='black',ls='--')\n",
    "    #ax3.axvline(t.value,color='black',ls='--')\n",
    "    \n",
    "    if False:\n",
    "        ax1.plot(c0.ewidth['Time'].value/1e6,c0.ewidth['Luminosity_H_A'],label=f'{t}')\n",
    "        ax2.plot(c0.FUV['Time'].value/1e6,c0.FUV['FUV'],label=f'{t}')   \n",
    "        ax3.plot(c0.FUV['Time'].value/1e6,c0.ewidth['Luminosity_H_A']/c0.FUV['FUV'],label=f'{t}')    \n",
    "\n",
    "    Ha += Ha_new\n",
    "    FUV += FUV_new\n",
    "    \n",
    "ax1.plot(time.value/1e6,Ha,label='sum')\n",
    "ax2.plot(time.value/1e6,FUV,label='sum')   \n",
    "ax3.plot(time.value/1e6,Ha/FUV,label='sum')   \n",
    "    \n",
    "ax1.set(xlabel='time / Myr',ylabel='Halpha',xlim=[0,20])\n",
    "ax2.set(xlabel='time / Myr',ylabel='FUV',xlim=[0,20])\n",
    "ax3.set(xlabel='time / Myr',ylabel='Halpha / FUV',xlim=[0,20])\n",
    "\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def __add__(table1,table2):\n",
    "    '''add two clusters with different ages\n",
    "    \n",
    "    this function takes the starburst\n",
    "    '''\n",
    "    \n",
    "    time1 = table1['Time']\n",
    "    \n",
    "    time2 = table2['Time']\n",
    "    \n",
    "    HI_rate = np.interp(time1,time2,table2['HI_rate'],left=0,right=0)\n",
    "    \n",
    "    plt.plot(time1,table1['HI_rate'])\n",
    "    plt.plot(time2,table2['HI_rate'])\n",
    "    plt.plot(time1,table1['HI_rate']+HI_rate)\n",
    "    plt.plot(time1,HI_rate)\n",
    "    plt.show()\n",
    "                                           \n",
    "    return 0\n",
    "    \n",
    "__add__(cluster.quanta,c2.quanta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Equ_width_H_A'],color='black')\n",
    "ax2.set(ylabel='eq width / Angstrom',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax3.plot(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "sc = ax3.scatter(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "                 c=cluster.ewidth['Time']/1e6,vmin=0,vmax=10)\n",
    "fig.colorbar(sc,ax=ax3,label='age / Myr')\n",
    "ax3.set(ylabel='Halpha/ FUV',xlabel='eq width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'SB99_age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of ionizing photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue.add_index('region_ID')\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "for region_ID in catalogue['region_ID']:\n",
    "    # the masses in the catalogue are off by a factor of 10\n",
    "    mass = 0.1*catalogue['mass'][catalogue['region_ID']==region_ID]\n",
    "    age  = catalogue['age'][catalogue['region_ID']==region_ID]*u.Myr\n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    idx = np.argmin(np.abs(scaled_cluster.quanta['Time']-age))\n",
    "    catalogue.loc[region_ID]['Qpredicted'] = scaled_cluster.quanta['HI_rate'][idx].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conversion factor is from Niederhofer+2016\n",
    "\n",
    "$$\n",
    "Q(\\mathrm{H}\\alpha) = 7.31\\cdot 10^{11} L(\\mathrm{H}\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed\n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_flux']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*Distance(distmod=p['(m-M)'])**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['age']<6) & (catalogue['age']>0)]\n",
    "# fesc = (Qpredicted-Qobserved) / Qpredicted \n",
    "fesc = (tmp['Qpredicted']-tmp['Qobserved'])/tmp['Qpredicted']\n",
    "\n",
    "print(f\"{np.sum(fesc<0)} of {len(tmp)} regions have negative fesc\")\n",
    "#Ha_from_q = (catalogue['Q']*1.37e-12*u.erg/u.s / (4*np.pi*Distance(distmod=p['(m-M)'])**2)).to(u.erg/u.s/u.cm**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "Qpredicted = np.logspace(-2,2)\n",
    "\n",
    "for f in [0.0,0.5,0.9,0.99]:\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    ax.plot(Qpredicted,Qobserved,label=f'fesc={f}',zorder=1)\n",
    "\n",
    "sc=ax.scatter(tmp['Qpredicted']/1e50,tmp['Qobserved']/1e50,\n",
    "           c=tmp['age'],cmap=plt.cm.copper,vmin=0,vmax=6,s=2,zorder=2)\n",
    "ax.legend()\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "#ax.plot([0,100],np.array([10,110]),color='gray',ls='--')\n",
    "#ax.plot([0,100],np.array([-10,90]),color='gray',ls='--')\n",
    "\n",
    "ax.set(xlabel=r'$Q_{\\mathrm{H}\\alpha}$ / $10^{50} \\mathrm{s}^{-1}$ predicted',\n",
    "       ylabel='$Q$ / $10^{50} \\mathrm{s}^{-1}$ observed',\n",
    "       xscale='log',yscale='log',xlim=[1e-2,1e2],ylim=[1e-2,1e2])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_fesc.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.hist(fesc,bins=np.arange(0,1.1,0.05))\n",
    "ax.set(xlim=[0,1.1],xlabel='fesc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.scatter(tmp['galactic_radius'],fesc)\n",
    "ax.set(ylim=[0,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Filter response curve\n",
    "\n",
    "to get the FUV flux by integrating the spectrum. The curves are from the [astrosat website](https://uvit.iiap.res.in/Instrument/Filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare Halpha and FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "FUV  = cluster.FUV['FUV']\n",
    "time_FUV = cluster.FUV['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha,color='tab:red')\n",
    "ax1.set_ylabel('Halpha/ (erg/s)',color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "FUV_int = np.interp(time_HA,time_FUV,FUV)\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='tab:blue')\n",
    "ax1.set_ylabel('Halpha / FUV',color='tab:blue')\n",
    "ax1.set(xlim=[0,3],ylim=[1e-5,0.0017],xlabel='Time/Myr')\n",
    "\n",
    "axt = ax1.twinx()\n",
    "quanta = cluster.quanta\n",
    "axt.plot(quanta['Time']/1e6,quanta['HI_rate'],color='tab:orange')\n",
    "axt.set_ylabel('ionizing photons / 1/s',color='tab:orange')\n",
    "axt.set(xlim=[0,10])\n",
    "\n",
    "HI_rate_int = np.interp(time_HA,quanta['Time'],quanta['HI_rate'])\n",
    "\n",
    "ax2.plot(HI_rate_int,Ha/FUV_int,color='black')\n",
    "sc = ax2.scatter(HI_rate_int,Ha/FUV_int,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(sc,cax=cax,label='age / Myr',pad=-1)\n",
    "\n",
    "ax2.set(xlabel='ionizing photons / 1/s',ylabel='Halpha / FUV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='black')\n",
    "#sc = ax1.scatter(FUV_int,Halpha,c=time_HA/1e6,vmin=0,vmax=3)\n",
    "#fig.colorbar(sc,ax=ax1,label='age / Myr')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [5e2,1e3,2e3,5e3,1e4,2e4,5e4]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    clusters[m] = Cluster(stellar_model=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "for m in [23,53,63]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax1.legend()\n",
    "\n",
    "for m in [24,54,64]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "\n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax2.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax2.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax2.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    ionizing = cl.quanta['HI_rate']\n",
    "  \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(Halpha/FUV,ionizing,color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(Halpha/FUV,ionizing,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / FUV',xlabel='ionization')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare observations to simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['distance'] = np.nan\n",
    "for name in np.unique(catalogue['gal_name']):\n",
    "    catalogue['distance'][catalogue['gal_name']==name] = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [2e3,5e3,1e4,2e4,5e4,1e5,2e5,5e5,1e6,2e6,5e6]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "Halpha_FLUX = ((catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "FUV_FLUX = 5e5*((catalogue['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "ax1.scatter(np.log10(FUV_FLUX),np.log10(Halpha_FLUX))\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the grid to compare the observations to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = len(cluster.ewidth['Time'])\n",
    "n_mass = 1000\n",
    "\n",
    "mass_min = 5e3\n",
    "mass_max = 1e6\n",
    "\n",
    "mass_grid = np.linspace(mass_min,mass_max,n_mass)\n",
    "\n",
    "HA_grid = np.zeros((n_time,n_mass))\n",
    "FUV_grid = np.zeros((n_time,n_mass))\n",
    "\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    HA_grid[:,i]  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    FUV_grid[:,i] = scaled_cluster.FUV['FUV']\n",
    "\n",
    "time = scaled_cluster.FUV['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)\n",
    "\n",
    "mass, age, chi2 = [],[],[]\n",
    "for row in catalogue:\n",
    "    \n",
    "    Halpha_FLUX = ((row['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    Halpha_ERR  = ((row['HA6562_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    FUV_FLUX = 1e6*((row['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    FUV_ERR  = ((row['FUV_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    chi2_grid = (Halpha_FLUX-HA_grid)**2/Halpha_ERR**2 + (FUV_FLUX-FUV_grid)**2/FUV_ERR**2\n",
    "    \n",
    "    row,col = np.unravel_index(chi2_grid.argmin(), chi2_grid.shape)\n",
    "    mass.append(mass_grid[col])\n",
    "    age.append(time[row].value)\n",
    "    chi2.append(np.min(chi2_grid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,catalogue['AGE_MINCHISQ'])\n",
    "ax1.set(ylim=[0,30],xlim=[0,30],xlabel='age from Nebulae / Myr',ylabel='age from Cluster / Myr')\n",
    "\n",
    "ax2.scatter(np.array(mass),catalogue['MASS_MINCHISQ'])\n",
    "ax2.set(xlabel='mass from Nebulae / Msun',ylabel='mass from Cluster / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(nrows=1,ncols=1,figsize=(single_column,single_column))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,np.array(mass))\n",
    "ax1.set(xlim=[0,30],ylim=[0,5e5],xlabel='age from Nebulae / Myr',ylabel='mass / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "eqwHA  = cluster.ewidth['Equ_width_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,eqwHA,color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr',ylabel='eq / AA',xlim=[0,10])\n",
    "\n",
    "#ax2 = ax1.twinx() \n",
    "#ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "#ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "#ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    eqwHA  = cl.ewidth['Equ_width_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    #sl = cl.scale(1e5)\n",
    "    #eqwHA  = sl.ewidth['Equ_width_H_A']\n",
    "    #time_HA = sl.ewidth['Time']\n",
    "    #ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),ls='--',color=c)\n",
    "    \n",
    "    \n",
    "ax1.set(xlabel='log (Time / Myr)',ylabel=r'log (W(H$\\alpha$) / $\\AA$)',xlim=[6,7.5])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    HAFUV  = cl.ewidth['Luminosity_H_A'] / cl.FUV['FUV']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    sl = cl.scale(1e5)\n",
    "    HAFUV  = sl.ewidth['Luminosity_H_A'] / sl.FUV['FUV']\n",
    "    time_HA = sl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,ls='--',color=c)    \n",
    "    \n",
    "ax1.set(xlabel='Time / Myr',ylabel=r'HA/FUV',xlim=[0,10])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(folder,**kwargs):\n",
    "\n",
    "    parameters = {\n",
    "    \"name\" : 'standard',\n",
    "    \"isf\" : -1,\n",
    "    \"mass\" : 1.,\n",
    "    \"sfr\" : 1,\n",
    "    \"ninterv\" : 2,\n",
    "    \"xponent\" : '1.3,2.3',\n",
    "    \"xmaslim\" : '0.1,0.5,120',\n",
    "    \"sncut\" : 8.,\n",
    "    \"bhcut\" : 120.,\n",
    "    \"model\" : 64,\n",
    "    \"wind_model\" : 0,\n",
    "    \"tinitial\" : 0.01,\n",
    "    \"time_scale\" : 0,\n",
    "    \"time_step\" : 0.1,\n",
    "    \"n_steps\" : 1000,\n",
    "    \"tmax\" : 50,\n",
    "    \"jmg\" : 3,\n",
    "    \"atmos\" : 5,\n",
    "    \"metallicity\" : 3,\n",
    "    \"uvline\" : 1}\n",
    "\n",
    "    # assign the new parameters\n",
    "    for k,v in kwargs.items():\n",
    "        if k in parameters:\n",
    "            parameters[k] = v\n",
    "    \n",
    "    # open tempalte\n",
    "    with open(basedir/'data'/'input.template') as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    template = template.format(**parameters)\n",
    "\n",
    "    #write to templae\n",
    "    with open(folder/'input.out','w') as f:\n",
    "        f.write(template)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age','mass','HA/FUV','HA6562_FLUX','region_area']\n",
    "\n",
    "\n",
    "data = np.zeros((len(catalogue),len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    data[:,i] = catalogue[col].data\n",
    "data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nebulae = PCA(n_components=2)\n",
    "principalComponents = pca_nebulae.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sitelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,4))\n",
    "\n",
    "for name,ax in zip(['NGC0628','NGC2835','NGC3351'],[ax1,ax2,ax3]):\n",
    "    with fits.open(basedir/'..'/'sitelle'/'data'/'maps'/f'{name}_OII_map.fits') as hdul:\n",
    "        OII = hdul['OII3726_FLUX'].data\n",
    "        OII_header = hdul['OII3726_FLUX'].header\n",
    "    \n",
    "    norm = simple_norm(OII,clip=False,percent=98)\n",
    "    ax.imshow(OII,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use [OII] line to calculate strong line and direct abundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.metallicity import diagnostic_line_ratios\n",
    "\n",
    "with fits.open(basedir/'..'/'sitelle'/'data'/'Nebulae_Catalogue_DR2_native_with_OII.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "\n",
    "nebulae = diagnostic_line_ratios(nebulae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with Figure 8 in Pilyugin+2016 (looks good if R2*=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.metallicity import strong_line_metallicity_R, strong_line_metallicity_S\n",
    "\n",
    "subsample = nebulae[nebulae['OII3726_FLUX_CORR']>10*nebulae['OII3726_FLUX_CORR_ERR']].copy()\n",
    "subsample['OH_R'] = strong_line_metallicity_R(1.4*subsample['R2'],subsample['R3'],subsample['N2'])\n",
    "subsample['OH_S'] = strong_line_metallicity_S(subsample['S2'],subsample['R3'],subsample['N2'])\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(6,3))\n",
    "\n",
    "ax1.plot([8.1,8.8],[8.1,8.8],color='black')\n",
    "ax1.plot([8.1,8.8],[8.2,8.9],color='grey',ls='--')\n",
    "ax1.plot([8.1,8.8],[8.0,8.7],color='grey',ls='--')\n",
    "ax1.scatter(subsample['OH_S'],subsample['OH_R'],s=4,c=tab10[0])\n",
    "ax1.set(xlim=[8.1,8.8],ylim=[8.1,8.8],\n",
    "       xlabel='12+log(O/H)$_\\mathrm{S}$',\n",
    "       ylabel='12+log(O/H)$_\\mathrm{R}$')\n",
    "ax2.hist(subsample['OH_S']-subsample['OH_R'],bins=np.linspace(-0.3,0.3,20),histtype='step',color='black')\n",
    "ax2.set(xlabel=r'log(O/H)$_\\mathrm{S}-$log(O/H)$_\\mathrm{R}$')\n",
    "plt.savefig('12+logOH R vs S calibration.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with direct method. This requires electron temperature and density. They have to be measured in an itterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.metallicity import electron_density_sulfur,\\\n",
    "                                electron_temperature_oxygen, electron_temperature_nitrogen,\\\n",
    "                                electron_temperature_sulfur, oxygen_abundance_direct\n",
    "   \n",
    "criteria = (nebulae['OII7319_FLUX_CORR']>7*nebulae['OII7319_FLUX_CORR_ERR']) & (nebulae['OII3726_FLUX_CORR']>10*nebulae['OII3726_FLUX_CORR_ERR'])\n",
    "subsample = nebulae[criteria].copy()\n",
    "subsample['OH_R'] = strong_line_metallicity_R(subsample['R2'],subsample['R3'],subsample['N2'])\n",
    "subsample['OH_S'] = strong_line_metallicity_S(subsample['S2'],subsample['R3'],subsample['N2'])\n",
    "    \n",
    "# initial guess for the temperature\n",
    "subsample['t(NII)'] = electron_temperature_nitrogen(subsample['RN2'])\n",
    "subsample['t(SIII)'] = electron_temperature_sulfur(subsample['RS3'])\n",
    "subsample['n(SII)']  = electron_density_sulfur(subsample['RS2'],subsample['t(NII)'])\n",
    "\n",
    "for x in range(10):\n",
    "    subsample['t(OII)'] = electron_temperature_oxygen(subsample['RO2'],subsample['n(SII)'])\n",
    "    subsample['n(SII)'] = electron_density_sulfur(subsample['RS2'],subsample['t(OII)'])\n",
    "    print(np.nanmean(subsample['n(SII)']))\n",
    "\n",
    "subsample['OH_direct'] = oxygen_abundance_direct(subsample['R2'],subsample['R3'],subsample['t(OII)'],subsample['n(SII)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(6,3))\n",
    "\n",
    "ax1.plot([8.1,8.8],[8.1,8.8],color='black')\n",
    "ax1.plot([8.1,8.8],[8.2,8.9],color='grey',ls='--')\n",
    "ax1.plot([8.1,8.8],[8.0,8.7],color='grey',ls='--')\n",
    "ax1.scatter(subsample['OH_direct'],subsample['OH_R'],s=4,c=tab10[0])\n",
    "ax1.set(xlim=[8.1,8.8],ylim=[8.1,8.8],\n",
    "       xlabel='12+log(O/H) direct',\n",
    "       ylabel='12+log(O/H)$_\\mathrm{R}$')\n",
    "ax2.hist(subsample['OH_direct']-subsample['OH_R'],bins=np.linspace(-0.3,0.3,20),histtype='step',color='black')\n",
    "ax2.set(xlabel=r'log(O/H) direct$-$log(O/H)$_\\mathrm{R}$')\n",
    "#plt.savefig('12+logOH R vs S calibration.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with Figure 7 in Perez-Montero+2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "\n",
    "ax.scatter(np.log10(subsample['R23']),subsample['OH_direct'],s=4,c=subsample['logq_D91'])\n",
    "ax.set(xlim=[-0.4,1.4],ylim=[7.,9.0],\n",
    "       xlabel='log R23',\n",
    "       ylabel='12+log(O/H) direct')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "ax.plot([0.5,1.5],[0.5,1.5],color='black')\n",
    "ax.scatter(subsample['t(OII)'],subsample['t(NII)'],s=4,c=tab10[0])\n",
    "ax.set(xlim=[0.5,1.5],ylim=[0.5,1.5],\n",
    "       xlabel='t([OII])',\n",
    "       ylabel='t([NII])')\n",
    "#plt.savefig('12+logOH R vs S calibration.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyNeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O3 = pn.Atom('O',3, NLevels=5)\n",
    "O2 = pn.Atom('O',2, NLevels=5)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,7))\n",
    "O2.plotGrotrian(ax=ax1)\n",
    "O3.plotGrotrian(ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pn.Observation()\n",
    "\n",
    "for line,wave in zip(['OI6300','OII3726','OII7319','OII7330','OIII5006','NII5754',\n",
    "             'NII6583','SII6716','SII6730','SIII6312','SIII9068'],\n",
    "               [6300,3726,7319,7330,5007,5755,6584,6716,6731,6312,9069], \n",
    "               ):\n",
    "    Intens = subsample[f'{line}_FLUX_CORR']\n",
    "    Error  = subsample[f'{line}_FLUX_CORR_ERR']\n",
    "    line = pn.EmissionLine(line[0],len(line[1:-4]),wave,obsIntens=Intens,obsError=Error)\n",
    "    \n",
    "    obs.addLine(line)\n",
    "    \n",
    "diags = pn.Diagnostics()\n",
    "diags.addDiagsFromObs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "emisgrids = pn.getEmisGridDict(atomDict=diags.atomDict)\n",
    "diags.plot(emisgrids, obs, ax=ax,i_obs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te,ne = diags.getCrossTemDen(diag_tem='[NII] 5755/6584',diag_den='[SII] 6731/6716',obs=obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = [10000.]\n",
    "Ne = [1e3]\n",
    "# Define a dictionary to hold all the Atom objects needed\n",
    "all_atoms = pn.getAtomDict(atom_list=obs.getUniqueAtoms())\n",
    "# define a dictionary to store the abundances\n",
    "ab_dict = {}\n",
    "# we  use the following lines to determine the ionic abundances\n",
    "ab_labels = ['O3_5007A', 'H1r_4861A', 'O2_3726A', 'O2_7319A', 'O2_7330A',\n",
    "             'N2_5755A', 'N2_6584A', 'S2_6716A', 'S2_6731A', 'S3_6312A','S3_9069A']\n",
    "\n",
    "for line in obs.getSortedLines():\n",
    "    if line.label in ab_labels:\n",
    "        ab = all_atoms[line.atom].getIonAbundance(line.corrIntens, Te, Ne, \n",
    "                                                  to_eval=line.to_eval, Hbeta=100)\n",
    "        ab_dict[line.atom] = ab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "idx,sep,_=match_coordinates_sky(associations['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "sep = sep.to(u.arcsec).value\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "bins = np.arange(0,10)\n",
    "\n",
    "print(f\"x<0.5: {np.mean(associations[sep<0.5]['age']):.2f}\")\n",
    "print(f\"0.5<x<1: {np.mean(associations[(sep>0.5) & (sep<1)]['age']):.2f}\")\n",
    "print(f\"1<x: {np.mean(associations[sep>1]['age']):.2f}\")\n",
    "\n",
    "ax.hist(associations[sep<0.5]['age'],bins=bins,alpha=0.5,label=r'$x<0.5\"$')\n",
    "ax.hist(associations[(sep>0.5) & (sep<1)]['age'],bins=bins,alpha=0.5,label=r'$0.5\"<x<1\"$')\n",
    "ax.hist(associations[sep>1]['age'],bins=bins,alpha=0.5,label=r'$1\"<x$')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.033px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
